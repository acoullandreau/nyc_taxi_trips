{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A data analysis of the NYC taxi rides - part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in part 1 that using a static visualisation for a flow, as well as loading the data into memory has its limits.\n",
    "In this notebook, we will go through the following steps required to generate animations and heat maps to visualise the flow of passengers of the yellow taxis in 2018:\n",
    "- set up the database\n",
    "- query the data\n",
    "- create an animation\n",
    "- create a heat map\n",
    "\n",
    "As a reminder, we are trying to answer the following questions:\n",
    "- Can we see trends in the flow of passengers in 2018?\n",
    "- Is there a difference on holidays, hottest or coldest day of the year?\n",
    "- Is there a difference between weekdays and weekends?\n",
    "- Depending on the zone we look at, where are people most likely to come from? To go to? Is it different between weekdays and weekends?\n",
    "\n",
    "What we are targeting is:\n",
    "- to have an animation for the whole year 2018, with 1-2 seconds of animation showing the flow of passengers every day (i.e the movement of dots on the map representing passengers going from one point to another)\n",
    "- to have an animation only for aggregated weekdays and weekends data - which would be represented by 1-2 seconds per week, with either just weekdays or just weekends.\n",
    "- to have a heat map showing for each zone the difference between the average of *incoming* passengers between weekdays and weekends (so for each zone we have a map, that uses a color code to show where people are mostly coming from, and whether it is more on weekdays or on weekends).\n",
    "- to have a heat map showing for each zone the difference between the average of *outgoing* passengers between weekdays and weekends (so for each zone we have a map, that uses a color code to show where people are mostly going to, and whether it is more on weekdays or on weekends).\n",
    "\n",
    "We should be able to identify trends from those visualizations in order to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Database set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment details and cleaning steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using MariaDB (5.7.18), we created three tables:\n",
    "- taxi_rides_2018 (primary key: auto-increment ID)\n",
    "- taxi_rides_2017 (primary key: increment ID)\n",
    "- taxi_zone_lookup_table (primary key: LocationID)\n",
    "\n",
    "As an idea of the size of the table, taxi_rides_2018 has about 97 million rows avec cleanup (about 100M beforehand). \n",
    "For both 2017 and 2018, we defined additional indexes to speed up the query:\n",
    "- PULocationID\n",
    "- DOLocationID\n",
    "- Date (column added)\n",
    "- Weekday (column added)\n",
    "\n",
    "We decided not to join the zone lookup table with the trips tables, but to do the join when querying the data.\n",
    "\n",
    "We performed a few cleaning steps:\n",
    "- Remove rows with values = 0 :\n",
    "    - PULocationID\n",
    "    - DOLocationID\n",
    "    - passenger_count\n",
    "    - tpep_pickup_datetime\n",
    "    - tpep_dropff_datetime\n",
    "- Remove rows with unused values:\n",
    "    - PULocationID equals to 264 or 265 (unknown zone id)\n",
    "    - DOLocationID equals to 264 or 265 (unknown zone id)\n",
    "- Remove rows with negative values:\n",
    "    - fare_amount\n",
    "    - extra\n",
    "    - mta_tax\n",
    "    - tip_amount\n",
    "    - tolls_amount\n",
    "    - improvement_surcharge\n",
    "- Add a column with only the date (as an index of the table)\n",
    "- Add a column with the weekday (as an index of the table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Queries for table creation and clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of the queries (example of 2017)\n",
    "\n",
    "#Create a table with all columns and indexes\n",
    "CREATE TABLE taxi_rides_2017 (\n",
    "    id INT NOT NULL AUTO_INCREMENT FIRST,\n",
    "    VendorID INTEGER NOT NULL,\n",
    "    tpep_pickup_datetime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    tpep_dropoff_datetime TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    pickup_date DATE NULL,\n",
    "    pickup_weekday INTEGER NOT NULL,\n",
    "    dropoff_date DATE NULL,\n",
    "    dropoff_weekday INTEGER NOT NULL,\n",
    "    passenger_count INTEGER NULL,\n",
    "    trip_distance FLOAT NULL,\n",
    "    RatecodeID INTEGER NOT NULL,\n",
    "    store_and_fwd_flag CHARACTER(1) NOT NULL,\n",
    "    PULocationID INTEGER NOT NULL,\n",
    "    DOLocationID INTEGER NOT NULL,\n",
    "    payment_type INTEGER NOT NULL,\n",
    "    fare_amount FLOAT NULL,\n",
    "    extra FLOAT NULL,\n",
    "    mta_tax FLOAT NULL,\n",
    "    tip_amount FLOAT NULL,\n",
    "    tolls_amount FLOAT NULL,\n",
    "    improvement_surcharge FLOAT NULL,\n",
    "    total_amount FLOAT NULL,\n",
    "    PRIMARY KEY (id),\n",
    "    INDEX pickup_date (pickup_date),\n",
    "    INDEX pickup_weekday (pickup_weekday),\n",
    "    INDEX dropoff_date (dropoff_date),\n",
    "    INDEX dropoff_weekday (dropoff_weekday),\n",
    "    FOREIGN KEY (PULocationID) REFERENCES taxi_zone_lookup_table(LocationID),\n",
    "    FOREIGN KEY (DOLocationID) REFERENCES taxi_zone_lookup_table(LocationID)\n",
    ");\n",
    "\n",
    "#Load the data - merged file for a year\n",
    "LOAD DATA LOCAL INFILE '/Users/acoullandreau/Desktop/Taxi_rides_DS/2017/merged_2017.csv' \n",
    "INTO TABLE taxi_rides_2017 \n",
    "FIELDS TERMINATED BY ',' \n",
    "LINES TERMINATED BY '\\r\\n'\n",
    "IGNORE 1 ROWS#Ignore header\n",
    "(VendorID,tpep_pickup_datetime,tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID,\tDOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount) \n",
    "SET id=null,#sets ID to auto-increment\n",
    "pickup_date = DATE(tpep_pickup_datetime),\n",
    "pickup_weekday = WEEKDAY(tpep_pickup_datetime), \n",
    "dropoff_date = DATE(tpep_dropoff_datetime), \n",
    "dropoff_weekday = WEEKDAY(tpep_dropoff_datetime)\n",
    ";\n",
    "\n",
    "#Clean up the data\n",
    "DELETE FROM nyc_taxi_rides.taxi_rides_2017 \n",
    "WHERE PULocationID IN (0, 264, 265) \n",
    "OR DOLocationID IN (0, 264, 265) \n",
    "OR passenger_count  = 0 \n",
    "OR tpep_pickup_datetime = 0 \n",
    "OR tpep_dropoff_datetime  = 0 \n",
    "OR fare_amount <0 \n",
    "OR extra<0 \n",
    "OR mta_tax<0 \n",
    "OR tip_amount<0 \n",
    "OR tolls_amount<0 \n",
    "OR improvement_surcharge<0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the database was up and ready, we could start writting the set of functions and the python script that would allow us to connect to the database, query the data, process it and render it, either in the form of an animation (video) or a heat map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Building a python script - part 1 (base map and query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:*\n",
    "\n",
    "*The code is documented in this notebook, with comments inside each function, as well as on GitHub, where both a documentation and a graph of connection of each function to the other are available. The idea of the graph is to illustrate the logical flow of the script.*\n",
    "\n",
    "To start with, we will need to:\n",
    "- process the shapefile\n",
    "- render a base map\n",
    "- prepare the query\n",
    "- connect to the database and execute the query\n",
    "\n",
    "The functions below can be used to go through each of these steps. \n",
    "\n",
    "Note that for simplification, we take as a reference for the date the *pick up* date. Looking at how many passengers leave one day and arrive the next (so travel around midnight), it is negligeable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import to be able to run the code below\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import shapefile as shp\n",
    "from pyproj import Proj, transform\n",
    "import cv2\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shp_to_df(sf):\n",
    " \n",
    "    fields = [x[0] for x in sf.fields][1:]\n",
    "    records = sf.records()\n",
    "    shps = [s.points for s in sf.shapes()]\n",
    "    df = pd.DataFrame(columns=fields, data=records)\n",
    "    df = df.assign(coords=shps)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_centroid(points):\n",
    "\n",
    "    x_sum = 0\n",
    "    y_sum = 0\n",
    "    for coords in points:\n",
    "        x_sum += coords[0]\n",
    "        y_sum += coords[1]\n",
    "        \n",
    "    x_mean = x_sum/len(points)\n",
    "    y_mean = y_sum/len(points)\n",
    "    \n",
    "    return x_mean, y_mean\n",
    "\n",
    "\n",
    "def calculate_boundaries(points):\n",
    "  \n",
    "    x_max = -99999999\n",
    "    x_min = 99999999\n",
    "    y_max = -99999999\n",
    "    y_min = 99999999\n",
    "    \n",
    "    for coords in points:\n",
    "        if coords[0] > x_max:\n",
    "            x_max = coords[0]\n",
    "        if coords[0] < x_min:\n",
    "            x_min = coords[0]\n",
    "        if coords[1] > y_max:\n",
    "            y_max = coords[1]\n",
    "        if coords[1] < y_min:\n",
    "            y_min = coords[1]\n",
    "        \n",
    "    max_bound = (x_max, y_max)\n",
    "    min_bound = (x_min, y_min)\n",
    "    \n",
    "    return max_bound, min_bound\n",
    "\n",
    "\n",
    "def process_shape_boundaries(df_sf, sf):\n",
    "    \n",
    "    shape_dict = {}\n",
    "    index_list = df_sf.index.tolist()\n",
    "    \n",
    "    for zone_id in index_list:\n",
    "        #for each zone id available in the shapefile\n",
    "        if zone_id not in shape_dict:\n",
    "            #we only process the coordinates if it is not yet included in the dictionary\n",
    "            shape_zone = sf.shape(zone_id)\n",
    "            \n",
    "            points = [(i[0], i[1]) for i in shape_zone.points]\n",
    "            \n",
    "            x_center, y_center = calculate_centroid(points)\n",
    "            max_bound, min_bound = calculate_boundaries(points)\n",
    "            \n",
    "            #we add to the dictionary, for the zone id, the shape boundaries as well\n",
    "            #as the coordinates of the center of the shape znd the zone extreme boundaries\n",
    "            shape_dict[zone_id] = {}\n",
    "            shape_dict[zone_id]['points'] = points\n",
    "            shape_dict[zone_id]['center'] = (x_center, y_center)\n",
    "            shape_dict[zone_id]['max_bound'] = max_bound\n",
    "            shape_dict[zone_id]['min_bound'] = min_bound\n",
    "            \n",
    "    return shape_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coords(shape_dict):\n",
    "    \n",
    "    all_max_bound = []\n",
    "    all_min_bound = []\n",
    "    \n",
    "    for zone in shape_dict:\n",
    "        zone_shape = shape_dict[zone]\n",
    "        max_bound_zone = zone_shape['max_bound']\n",
    "        min_bound_zone = zone_shape['min_bound']\n",
    "        all_max_bound.append(max_bound_zone)\n",
    "        all_min_bound.append(min_bound_zone)\n",
    "    \n",
    "    map_max_bound, unused_max = calculate_boundaries(all_max_bound)\n",
    "    unused_min, map_min_bound = calculate_boundaries(all_min_bound)\n",
    "    \n",
    "    return map_max_bound, map_min_bound\n",
    "\n",
    "\n",
    "def define_projection(map_max_bound, map_min_bound, image_size):\n",
    "    \n",
    "    #We get the max 'coordinates' for both the target image and the shape we want to draw\n",
    "    image_x_max = image_size[0]\n",
    "    image_y_max = image_size[1]\n",
    "    map_x_max = map_max_bound[0]\n",
    "    map_y_max = map_max_bound[1]\n",
    "    map_x_min = map_min_bound[0]\n",
    "    map_y_min = map_min_bound[1]\n",
    "    \n",
    "    projection = {}\n",
    "\n",
    "    #we check which size is bigger to know based on which axis we want to scale our shape to\n",
    "    #we do the comparison using the aspect ratio expectations (dividing each axis by the\n",
    "    #size of the target axis in the new scale)\n",
    "    if (map_x_max - map_x_min)/image_x_max > (map_y_max - map_y_min)/image_y_max:\n",
    "        conversion = image_x_max / (map_x_max - map_x_min)\n",
    "        axis_to_center = 'y'#we store the axis we will want to center on based on which\n",
    "        #axis we perform the scaling from\n",
    "    else:\n",
    "        conversion = image_y_max / (map_y_max - map_y_min)\n",
    "        axis_to_center = 'x'\n",
    "\n",
    "    projection['image_size'] = image_size\n",
    "    projection['map_max_bound'] = map_max_bound\n",
    "    projection['map_min_bound'] = map_min_bound\n",
    "    projection['conversion'] = conversion\n",
    "    projection['axis_to_center'] = axis_to_center\n",
    "    \n",
    "    return projection\n",
    "\n",
    "\n",
    "def convert_projection(x, y, projection, inverse=False):\n",
    "    \n",
    "    x_min = projection['map_min_bound'][0]\n",
    "    y_min = projection['map_min_bound'][1]\n",
    "    conversion = projection['conversion']\n",
    "    \n",
    "    if inverse == False:\n",
    "        #to be able to center the image, we first translate the coordinates to the origin\n",
    "        x = (x - x_min) *conversion\n",
    "        y = (y - y_min) *conversion\n",
    "    else:\n",
    "        x = (x + x_min) /conversion\n",
    "        y = (y + y_min) /conversion\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def convert_shape_boundaries(zone_shape_dict, projection):\n",
    "    \n",
    "    converted_dict = {}\n",
    "    axis_to_center = projection['axis_to_center']\n",
    "    image_x_max = projection['image_size'][0]\n",
    "    image_y_max = projection['image_size'][1]\n",
    "    map_max_bound_converted = (convert_projection(projection['map_max_bound'][0], projection['map_max_bound'][1], projection))\n",
    "    map_min_bound_converted = (convert_projection(projection['map_min_bound'][0], projection['map_min_bound'][1], projection))\n",
    "    \n",
    "    if axis_to_center == 'x':\n",
    "        center_translation = (image_x_max - (map_max_bound_converted[0] - map_min_bound_converted[0]))/2\n",
    "    else:\n",
    "        center_translation = (image_y_max - (map_max_bound_converted[1] - map_min_bound_converted[1]))/2\n",
    "    \n",
    "    \n",
    "    for zone_id in zone_shape_dict:\n",
    "        curr_shape = zone_shape_dict[zone_id]\n",
    "        \n",
    "        points = curr_shape['points']\n",
    "        x_center = curr_shape['center'][0]\n",
    "        y_center = curr_shape['center'][1]\n",
    "        max_bound = curr_shape['max_bound']\n",
    "        min_bound = curr_shape['min_bound']\n",
    "        \n",
    "        converted_points = []\n",
    "        for point in points:\n",
    "            #we convert the coordinates to the new coordinate system\n",
    "            converted_point = [0, 0] \n",
    "            converted_point[0], converted_point[1] = convert_projection(point[0], point[1], projection)\n",
    "            #we center the map on the axis that was not used to scale the image\n",
    "            if axis_to_center == 'x':\n",
    "                converted_point[0] = converted_point[0] + center_translation\n",
    "            else:\n",
    "                converted_point[1] = converted_point[1] + center_translation\n",
    "            \n",
    "            #we mirror the image to match the axis alignment\n",
    "            converted_point[1] = image_y_max - converted_point[1]\n",
    "            converted_points.append(converted_point)\n",
    "        \n",
    "        #we convert the center and the max and min boundaries\n",
    "        x_center, y_center = calculate_centroid(converted_points)\n",
    "        max_bound = (convert_projection(max_bound[0], max_bound[1], projection))\n",
    "        min_bound = (convert_projection(min_bound[0], min_bound[1], projection))\n",
    "        \n",
    "        \n",
    "        #We edit the dictionary with the new coordinates\n",
    "        converted_dict[zone_id] = {}\n",
    "        converted_dict[zone_id]['points'] = converted_points\n",
    "        converted_dict[zone_id]['center'] = (x_center, y_center)\n",
    "        converted_dict[zone_id]['max_bound'] = max_bound\n",
    "        converted_dict[zone_id]['min_bound'] = min_bound\n",
    "        \n",
    "    return converted_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_set_to_draw(map_type, shape_dict, df_sf, image_size):\n",
    "\n",
    "    #we define if we want to draw the whole map or only a borough (in this case map_type\n",
    "    #should be the borough name)\n",
    "    if map_type == 'total':\n",
    "        shape_dict = shape_dict\n",
    "    else:\n",
    "        #we select the list of zone_id we want to draw that belong only to the targeted \n",
    "        #borough to draw\n",
    "        shape_dict = reduce_shape_dict_to_borough(shape_dict, df_sf, map_type)\n",
    "    \n",
    "    #We define the projection parameters to be able to convert the coordinates into\n",
    "    #the image scale coordinate system\n",
    "    #we convert the coordinates of the shapes to draw\n",
    "    map_max_bound, map_min_bound = find_max_coords(shape_dict)\n",
    "    projection = define_projection(map_max_bound, map_min_bound, image_size)\n",
    "    converted_shape_dict = convert_shape_boundaries(shape_dict, projection)\n",
    "    \n",
    "    return converted_shape_dict, projection\n",
    "\n",
    "\n",
    "def reduce_shape_dict_to_borough(shape_dict, df_sf, borough_name):\n",
    "        \n",
    "    borough_df = df_sf[df_sf['borough']==borough_name]\n",
    "    borough_id = []\n",
    "    for objectid in borough_df.index:\n",
    "        borough_id.append(objectid)\n",
    "    \n",
    "    reduced_shape_dict = {}\n",
    "    #we add to the reduced_shape_dict only the zones belonging to the borough area targeted\n",
    "    for zone_id in borough_id:\n",
    "        reduced_shape_dict[zone_id] = shape_dict[zone_id]\n",
    "    \n",
    "    return reduced_shape_dict\n",
    "\n",
    "\n",
    "def draw_base_map(draw_dict):\n",
    "    \n",
    "    #We extract the variables we will need from the input dictionary\n",
    "    image_size = draw_dict['image_size']\n",
    "    map_type = draw_dict['map_type']\n",
    "    title = draw_dict['title']\n",
    "    shape_dict = draw_dict['shape_dict']\n",
    "    df_sf = draw_dict['df_sf']\n",
    "    render_single_borough = draw_dict['render_single_borough']\n",
    "                    \n",
    "    #first we create a blank image, on which we will draw the base map\n",
    "    width = image_size[0]\n",
    "    height = image_size[1]\n",
    "    base_map = np.zeros((height,width,3), np.uint8) #Size of the image 1080 height, 1920 width, 3 channels of colour\n",
    "    base_map[:, :] = [0, 0, 0] #Sets the color to white\n",
    "    \n",
    "    #we isolate the set of shapes we want to draw in the right coordinate system\n",
    "    converted_shape_dict, projection = get_shape_set_to_draw(map_type, shape_dict, df_sf, image_size)\n",
    "    \n",
    "    if render_single_borough == False:\n",
    "        #we use the projection parameters from the borough we want to focus on\n",
    "        #we calculate the coordinates for the whole map\n",
    "        converted_shape_dict = convert_shape_boundaries(shape_dict, projection)\n",
    "        \n",
    "    #we draw each shape of the dictionary on the blank image, \n",
    "    #either the full map or only a borough \n",
    "    for item in converted_shape_dict:\n",
    "        shape = converted_shape_dict[item]\n",
    "        points = shape['points']\n",
    "        pts = np.array(points, np.int32)\n",
    "        cv2.polylines(base_map, [pts], True, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    #we display general text information   \n",
    "    display_general_information_text(base_map, map_type, title)\n",
    "    \n",
    "    return base_map, projection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sql_query(query_dict):\n",
    "\n",
    "    #We extract the variables we will need from the input dictionary\n",
    "    data_table = query_dict['data_table']\n",
    "    lookup_table = query_dict['lookup_table']\n",
    "    aggregated_result = query_dict['aggregated_result']\n",
    "    date = query_dict['date']\n",
    "    filter_query_on_borough = query_dict['filter_query_on_borough']\n",
    "    aggregate_period = query_dict['aggregate_period']\n",
    "    weekdays = query_dict['weekdays']\n",
    "    \n",
    "    #first we synthesise what we want to fetch\n",
    "    if aggregated_result == 'count':\n",
    "        aggregated_result = 'COUNT(passenger_count)'\n",
    "    elif aggregated_result == 'avg':\n",
    "        aggregated_result = 'AVG(passenger_count)'\n",
    "    \n",
    "\n",
    "    #then we work on the 'WHERE' statements and the JOIN \n",
    "    if aggregate_period == True:\n",
    "        start_date = date[0]\n",
    "        end_date = date[1]\n",
    "        \n",
    "        if weekdays == ():\n",
    "            if filter_query_on_borough != False:\n",
    "                query = (\"SELECT pu_id, do_id, aggregated_result FROM (\\\n",
    "                            SELECT PULocationID pu_id, DOLocationID do_id, {0} aggregated_result\\\n",
    "                            FROM {1} tr_2018\\\n",
    "                            WHERE pickup_date BETWEEN '{2}' AND '{3}'\\\n",
    "                            GROUP BY pu_id, do_id\\\n",
    "                            ORDER by aggregated_result\\\n",
    "                        ) AS tr_2018\\\n",
    "                         JOIN {4} lookup_pu\\\n",
    "                         ON lookup_pu.LocationID = tr_2018.pu_id \\\n",
    "                         JOIN {4} lookup_do \\\n",
    "                         ON lookup_do.LocationID = tr_2018.do_id \\\n",
    "                         WHERE lookup_pu.borough_name = '{5}' AND lookup_do.borough_name = '{5}'\".format\n",
    "                        (aggregated_result, data_table, start_date, end_date, lookup_table, filter_query_on_borough))\n",
    "\n",
    "            else:\n",
    "                query = (\"SELECT PULocationID pu_id, DOLocationID do_id, {0} aggregated_result\\\n",
    "                            FROM {1} AS tr_2018\\\n",
    "                            WHERE pickup_date BETWEEN '{2}' AND '{3}'\\\n",
    "                            GROUP BY pu_id, do_id\".format(aggregated_result, data_table, start_date, end_date))\n",
    "        \n",
    "        else:\n",
    "            if filter_query_on_borough != False:\n",
    "                query = (\"SELECT pu_id, do_id, aggregated_result FROM (\\\n",
    "                            SELECT PULocationID pu_id, DOLocationID do_id, {0} aggregated_result\\\n",
    "                            FROM {1} tr_2018\\\n",
    "                            WHERE pickup_date BETWEEN '{2}' AND '{3}' AND pickup_weekday IN {4}\\\n",
    "                            GROUP BY pu_id, do_id\\\n",
    "                            ORDER by aggregated_result\\\n",
    "                        ) AS tr_2018\\\n",
    "                         JOIN {5} lookup_pu\\\n",
    "                         ON lookup_pu.LocationID = tr_2018.pu_id \\\n",
    "                         JOIN {5} lookup_do \\\n",
    "                         ON lookup_do.LocationID = tr_2018.do_id \\\n",
    "                         WHERE lookup_pu.borough_name = '{6}' AND lookup_do.borough_name = '{6}'\".format\n",
    "                        (aggregated_result, data_table, start_date, end_date, weekdays, lookup_table, filter_query_on_borough))\n",
    "\n",
    "            else:\n",
    "                query = (\"SELECT PULocationID pu_id, DOLocationID do_id, {0} aggregated_result\\\n",
    "                            FROM {1} AS tr_2018\\\n",
    "                            WHERE pickup_date BETWEEN '{2}' AND '{3}' AND pickup_weekday IN {4}\\\n",
    "                            GROUP BY pu_id, do_id\".format(aggregated_result, data_table, start_date, end_date, weekdays))\n",
    "        \n",
    "    else:\n",
    "        if filter_query_on_borough != False:\n",
    "            query = (\"SELECT pu_id, do_id, aggregated_result FROM (\\\n",
    "                        SELECT PULocationID pu_id, DOLocationID do_id, {0} aggregated_result\\\n",
    "                        FROM {1} tr_2018\\\n",
    "                        WHERE pickup_date = '{2}'\\\n",
    "                        GROUP BY pu_id, do_id\\\n",
    "                        ORDER by aggregated_result\\\n",
    "                    ) AS tr_2018\\\n",
    "                     JOIN {3} lookup_pu\\\n",
    "                     ON lookup_pu.LocationID = tr_2018.pu_id \\\n",
    "                     JOIN {3} lookup_do \\\n",
    "                     ON lookup_do.LocationID = tr_2018.do_id \\\n",
    "                     WHERE lookup_pu.borough_name = '{4}' AND lookup_do.borough_name = '{4}'\".format\n",
    "                    (aggregated_result, data_table, date, lookup_table, filter_query_on_borough))\n",
    "\n",
    "        else:\n",
    "            query = (\"SELECT PULocationID pu_id, DOLocationID do_id, {0} aggregated_result\\\n",
    "                        FROM {1} AS tr_2018\\\n",
    "                        WHERE pickup_date = '{2}'\\\n",
    "                        GROUP BY pu_id, do_id\".format(aggregated_result, data_table, date))        \n",
    "\n",
    "    return query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sql_query(query, database):\n",
    "\n",
    "    #connect to the database\n",
    "    db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        passwd=\"password\",\n",
    "        database=database\n",
    "        )\n",
    "\n",
    "    #execute the query...\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # ...and store the output\n",
    "    results=[]\n",
    "    for result in cursor:\n",
    "        results.append(result)\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Building a python script - part 2 (animation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we havea base map and the result of the query, we are able to start processing the data to render it in an animation.\n",
    "\n",
    "We will need to:\n",
    "- process the query results\n",
    "- render each point by interpolating its position\n",
    "- render each frame\n",
    "- render the animation\n",
    "\n",
    "The functions below can be used to go through each of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_next_position(origin_coords, destination_coords, tot_frames, curr_frame):\n",
    "        \n",
    "    #as to perform the arithmetic operations, we convert everything to float for more \n",
    "    #precision\n",
    "    x_origin = float(origin_coords[0])\n",
    "    y_origin = float(origin_coords[1])\n",
    "    x_destination = float(destination_coords[0])\n",
    "    y_destination = float(destination_coords[1])\n",
    "    tot_frames = float(tot_frames - 1)\n",
    "    curr_frame = float(curr_frame)\n",
    "    \n",
    "    delta_x = (x_destination - x_origin)/tot_frames\n",
    "    delta_y = (y_destination - y_origin)/tot_frames\n",
    "    \n",
    "    #the rendering with OpenCV demands integers values for the positioning, so we convert\n",
    "    #w and y to int\n",
    "    new_x = int(x_origin+delta_x*curr_frame)\n",
    "    new_y = int(y_origin+delta_y*curr_frame)\n",
    "    \n",
    "    return new_x, new_y\n",
    "\n",
    "\n",
    "\n",
    "def render_point_on_map(x_point, y_point, weight, base_map, colour):\n",
    "      \n",
    "    cv2.circle(base_map, (x_point,y_point), weight, colour, -1)\n",
    "    \n",
    "    \n",
    "def convert_id_shape(idx, inverse = False):\n",
    "        \n",
    "    if inverse == False:\n",
    "        idx = idx - 1\n",
    "    else:\n",
    "        idx = idx + 1\n",
    "    \n",
    "    return idx\n",
    "    \n",
    "\n",
    "def build_query_dict(render_animation_dict):\n",
    "    \n",
    "    #First, we extract the variables we will need from the input dictionary\n",
    "    time_granularity = render_animation_dict['time_granularity']\n",
    "    filter_query_on_borough = render_animation_dict['filter_query_on_borough']\n",
    "    weekdays = render_animation_dict['weekdays']\n",
    "    \n",
    "    #we instantiate the query_dict and start filling it with query parameters\n",
    "    query_dict = {}\n",
    "    query_dict['data_table'] = render_animation_dict['data_table']\n",
    "    query_dict['lookup_table'] = render_animation_dict['lookup_table']\n",
    "    query_dict['aggregated_result'] = render_animation_dict['aggregated_result']\n",
    "    query_dict['aggregate_period'] = render_animation_dict['aggregate_period']\n",
    "    query_dict['weekdays'] = weekdays\n",
    "    \n",
    "    #we handle the borough related WHEN statement\n",
    "    if filter_query_on_borough == False:\n",
    "        query_dict['filter_query_on_borough'] = False\n",
    "    else:\n",
    "        query_dict['filter_query_on_borough'] = filter_query_on_borough\n",
    "    \n",
    "    #we handle the time related WHEN statements\n",
    "    period = render_animation_dict['period']\n",
    "    start_date = period[0]\n",
    "    end_date = period[1]\n",
    "        \n",
    "    if start_date == end_date:\n",
    "        query_dict['date'] = start_date\n",
    "\n",
    "    else:\n",
    "        #if the period is more than one date, we will have to loop through the\n",
    "        #date range and render multiple series of 60 frames (1 second at 60 fps per day)\n",
    "        #Thus the loop needs to be handled by the main plotting function, and here we\n",
    "        #simply add a flag to the query dict that will be transformed by the plotting\n",
    "        #function\n",
    "        query_dict['date'] = 'loop_through_period'\n",
    "\n",
    "    #used specifically for the animation logic\n",
    "    if time_granularity == 'specific_weekdays' or weekdays !=():\n",
    "        specific_weekdays = render_animation_dict['weekdays']\n",
    "        query_dict['specific_weekdays'] = 'on_specific_weekdays'\n",
    "    \n",
    "    #used specifically for the animation logic\n",
    "    elif time_granularity == 'period':\n",
    "        query_dict['specific_weekdays'] = False\n",
    "    \n",
    "    #used specifically for the heat_map logic\n",
    "    elif time_granularity == 'weekdays_vs_weekends':\n",
    "        query_dict['specific_weekdays'] = 'weekdays_vs_weekends'\n",
    "    \n",
    "    return query_dict\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_max_passengers(trips_list, idx_weight):\n",
    "\n",
    "    min_passenger_itinerary = min(trips_list, key=lambda x:x[idx_weight])\n",
    "    max_passenger_itinerary = max(trips_list, key=lambda x:x[idx_weight])\n",
    "    max_passenger = max_passenger_itinerary[idx_weight]\n",
    "    min_passenger = min_passenger_itinerary[idx_weight]\n",
    "    \n",
    "    return min_passenger, max_passenger\n",
    "\n",
    "\n",
    "def compute_weight(map_type, weight, max_passenger):\n",
    "    #we normalise the weight of the point based on the max number of passengers\n",
    "    #which means that from one day to another, although the biggest point will have the\n",
    "    #same size, it will not represent the same number of passengers (compromise to\n",
    "    #prevent having huge differences between the points, or squishing too much the scale\n",
    "    #by using a log). \n",
    "    \n",
    "    if map_type != 'total':\n",
    "        weight = weight/max_passenger*30\n",
    "    else:\n",
    "        weight = weight/max_passenger*20\n",
    "\n",
    "    weight = int(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def display_specific_text_animation(rendered_frame, date_info, map_type, min_pass, max_pass):\n",
    "    #note that these position are based on an image size of [1920, 1080]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    agg_per = date_info[0]\n",
    "    date = date_info[1]\n",
    "    \n",
    "    #displays the date and the weekday, and if it is a special date\n",
    "    if agg_per == False:\n",
    "        cv2.putText(rendered_frame, date, (40, 150), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "\n",
    "        special_dates_2018 = {'2018-01-01':'New Year', '2018-12-25':'Christmas',\n",
    "                             '2018-02-14':'Valentine\\'s Day', '2018-07-04':'National Day',\n",
    "                             '2018-07-01':'Hottest Day', '2018-01-07':'Coldest Day'}\n",
    "        if date in special_dates_2018:\n",
    "            cv2.putText(rendered_frame, special_dates_2018[date], (40, 200), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "\n",
    "        date_timestamp = pd.Timestamp(date)\n",
    "        weekday = date_timestamp.dayofweek\n",
    "        weekdays = {0:'Monday', 1:'Tuesday',2:'Wednesday',3:'Thursday', \n",
    "                    4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "        weekday = weekdays[weekday]\n",
    "        cv2.putText(rendered_frame, weekday, (40, 95), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(rendered_frame, 'Week of the {}'.format(date), (40, 150), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    #displays the legend of the colour code\n",
    "    cv2.putText(rendered_frame, 'Origin and destination', (35, 260), font, 0.8, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    cv2.circle(rendered_frame, (40, 290), 10, (141, 91, 67), -1)\n",
    "    cv2.putText(rendered_frame, 'Identical', (60, 300), font, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    cv2.circle(rendered_frame, (40, 320), 10, (135,162,34), -1)\n",
    "    cv2.putText(rendered_frame, 'Distinct', (60, 330), font, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    #displays the legend of the size of the circles\n",
    "    cv2.putText(rendered_frame, 'Number of passengers', (35, 380), font, 0.8, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    max_weight = compute_weight(map_type, max_pass, max_pass)\n",
    "    cv2.circle(rendered_frame, (40, 420), max_weight, (255, 255, 255), 1)\n",
    "    cv2.putText(rendered_frame, '{} passengers'.format(max_pass), (80, 420), font, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    min_weight = compute_weight(map_type, min_pass, max_pass)\n",
    "    cv2.circle(rendered_frame, (40, 460), min_weight, (255, 255, 255), 1)\n",
    "    cv2.putText(rendered_frame, '{} passenger'.format(min_pass), (80, 460), font, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "\n",
    "def display_general_information_text(image, map_type, video_title):\n",
    "\n",
    "    #note that these position are based on an image size of [1920, 1080]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    #displays the name of the boroughs of the city\n",
    "    if map_type == 'total':\n",
    "        #name of borough Manhattan\n",
    "        cv2.putText(image, 'Manhattan', (770, 360), \n",
    "            font, 0.8, (255, 255, 255), 1, cv2.LINE_AA) \n",
    "        #name of borough Brooklyn\n",
    "        cv2.putText(image, 'Brooklyn', (1130, 945), \n",
    "            font, 0.8,(255, 255, 255), 1, cv2.LINE_AA) \n",
    "        #name of borough Staten Island\n",
    "        cv2.putText(image, 'Staten Island', (595, 1030), \n",
    "            font, 0.8, (255, 255, 255), 1, cv2.LINE_AA) \n",
    "        #name of borough Queens\n",
    "        cv2.putText(image, 'Queens', (1480, 590), \n",
    "            font, 0.8, (255, 255, 255), 1, cv2.LINE_AA) \n",
    "        #name of borough Bronx\n",
    "        cv2.putText(image, 'Bronx', (1370, 195), \n",
    "            font, 0.8, (255, 255, 255), 1, cv2.LINE_AA) \n",
    "    \n",
    "    else:\n",
    "        video_title = video_title + ' in ' + map_type\n",
    "\n",
    "    #displays the title of the video\n",
    "    #cv2.putText(image, video_title, (500, 1050), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "\n",
    "def render_frame(frame, base_map, query_results, max_passenger, converted_shape_dict, map_type):\n",
    " \n",
    "    #we make a copy of the map on which we will render the frame (each frame being\n",
    "    #rendered on a new copy)\n",
    "    map_rendered = base_map.copy()\n",
    "    \n",
    "    #we get each tuple from the query result, in the form (origin_id, dest_id, weight)\n",
    "    for itinerary in query_results:\n",
    "        zone_id_origin = convert_id_shape(itinerary[0])\n",
    "        zone_id_destination = convert_id_shape(itinerary[1])\n",
    "\n",
    "        weight = itinerary[2]\n",
    "        weight = compute_weight(map_type, weight, max_passenger)\n",
    "\n",
    "        #we get the coordinates of the center of the origin and the destination\n",
    "        origin_coords = converted_shape_dict[zone_id_origin]['center']\n",
    "        destination_coords = converted_shape_dict[zone_id_destination]['center']\n",
    "\n",
    "        if frame == 0:\n",
    "            #we start the rendering with the point at the origin\n",
    "            #we convert to int as to be able to plot the point with opencv\n",
    "            coords_point_to_draw = (int(origin_coords[0]), int(origin_coords[1]))\n",
    "\n",
    "        else:\n",
    "            #we extrapolate the position of the point between the origin and the\n",
    "            #destination, as to have the point move from origin to destination\n",
    "            #in 60 frames\n",
    "            coords_point_to_draw = interpolate_next_position(origin_coords, destination_coords, 60, frame)\n",
    "\n",
    "        x_point = coords_point_to_draw[0]\n",
    "        y_point = coords_point_to_draw[1]\n",
    "\n",
    "        if zone_id_origin == zone_id_destination:\n",
    "            colour = (141, 91, 67)\n",
    "        else:\n",
    "            colour = (135,162,34)\n",
    "\n",
    "        render_point_on_map(x_point, y_point, weight, map_rendered, colour)\n",
    "\n",
    "    return map_rendered\n",
    "\n",
    "\n",
    "def render_all_frames(render_frame_dict):\n",
    "    \n",
    "    #we extract the arguments we need from the input dictionary\n",
    "    query_date = render_frame_dict['query_date']\n",
    "    query_results = render_frame_dict['query_results']\n",
    "    database = render_frame_dict['database']\n",
    "    base_map = render_frame_dict['base_map']\n",
    "    converted_shape_dict = render_frame_dict['converted_shape_dict']\n",
    "    map_type = render_frame_dict['map_type']\n",
    "    frames = render_frame_dict['frames']\n",
    "    min_pass = render_frame_dict['min_passenger']\n",
    "    max_pass = render_frame_dict['max_passenger']\n",
    "    agg_per = render_frame_dict['agg_per']\n",
    "\n",
    "    #we use the results of the query to render 60 frames\n",
    "    #we want to render an animation of 1 second per given date, at 60 fps. \n",
    "    for frame in range(0, 60):  \n",
    "        rendered_frame = render_frame(frame, base_map, query_results, max_pass, \n",
    "                                        converted_shape_dict, map_type)       \n",
    "        \n",
    "        #we display frame related text\n",
    "        date_info = (agg_per, query_date)\n",
    "        display_specific_text_animation(rendered_frame, date_info, map_type, min_pass, max_pass)\n",
    "        \n",
    "        frames.append(rendered_frame)\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "def make_video_animation(frames, image_size, map_type):\n",
    "        \n",
    "    #Build the title for the animation\n",
    "    if map_type == 'total':\n",
    "        title = 'Animation_{}.avi'.format('NYC')\n",
    "    else:\n",
    "        title = 'Animation_{}.avi'.format(map_type)\n",
    "    \n",
    "    animation = cv2.VideoWriter(title, cv2.VideoWriter_fourcc(*'DIVX'), 30, image_size)\n",
    "    #video title, codec, fps, frame size\n",
    "\n",
    "    for i in range(len(frames)):\n",
    "        animation.write(frames[i])\n",
    "    \n",
    "    animation.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_arg(render_animation_dict):\n",
    "    period = render_animation_dict['period'] \n",
    "    query_dict = render_animation_dict['query_dict']\n",
    "    database = render_animation_dict['database']\n",
    "    specific_weekdays = query_dict['specific_weekdays']\n",
    "    date = query_dict['date']\n",
    "    aggregate_period = render_animation_dict['aggregate_period']\n",
    "    weekdays = render_animation_dict['weekdays']\n",
    "\n",
    "    query_results_dict = {}\n",
    "    \n",
    "    if aggregate_period == False and query_dict['date'] == 'loop_through_period':\n",
    "        #in this case we want the result for each day of the period provided\n",
    "        #if we have the flag loop_through_period in the query dict, it means the period\n",
    "        #set for the query is multiple dates\n",
    "        \n",
    "        daterange = pd.date_range(period[0],period[1])\n",
    "        #we run queries for each date in the daterange specified\n",
    "        for single_date in daterange: \n",
    "            date = pd.to_datetime(single_date)\n",
    "            if specific_weekdays == 'on_specific_weekdays':\n",
    "                \n",
    "                #we check if the date of the daterange matches the weekday(s) we target\n",
    "                if date.dayofweek in weekdays:\n",
    "                    single_date = date.date().strftime('%Y-%m-%d')\n",
    "                    query_dict['date'] = single_date\n",
    "                    query = prepare_sql_query(query_dict)\n",
    "                    query_results = make_sql_query(query, database)\n",
    "                    query_results_dict[query_dict['date']] = query_results\n",
    "\n",
    "                else:\n",
    "                    #if a date in the range is not among the weekdays we want, we skip it\n",
    "                    continue\n",
    "            else:\n",
    "                single_date = date.date().strftime('%Y-%m-%d')\n",
    "                query_dict['date'] = single_date\n",
    "                query = prepare_sql_query(query_dict)\n",
    "                query_results = make_sql_query(query, database)\n",
    "                query_results_dict[query_dict['date']] = query_results\n",
    "\n",
    "\n",
    "    elif aggregate_period == True and query_dict['date'] == 'loop_through_period':\n",
    "        #in this case, we want to aggregate the results (sum) per week\n",
    "        daterange = pd.date_range(period[0],period[1])\n",
    "        start_date = pd.to_datetime(period[0])\n",
    "        end_date = pd.to_datetime(period[1])\n",
    "        \n",
    "        #let's build a list of all intervals we will want to aggregate the data for\n",
    "        all_aggr_init = []\n",
    "        start = start_date\n",
    "        end = end_date\n",
    "        \n",
    "        #we add one list of dates per week to the list of all intervals\n",
    "        i = 0\n",
    "        for date in daterange:\n",
    "            #we handle separately the first date of the period\n",
    "            if i == 0:\n",
    "                curr_week = [start.date().strftime('%Y-%m-%d')]\n",
    "\n",
    "            if date != start_date and date !=end_date:\n",
    "                start_week_number = start.isocalendar()[1]\n",
    "                date_week_number = date.isocalendar()[1]\n",
    "\n",
    "                if date_week_number == start_week_number:\n",
    "                    curr_week.append(date.date().strftime('%Y-%m-%d'))\n",
    "                    i+=1\n",
    "                else:\n",
    "                    start = date\n",
    "                    all_aggr_init.append(curr_week)\n",
    "                    i = 0\n",
    "\n",
    "                \n",
    "        #we handle separately the last date of the period\n",
    "        if curr_week not in all_aggr_init:\n",
    "            curr_week.append(end_date.date().strftime('%Y-%m-%d'))\n",
    "            all_aggr_init.append(curr_week)\n",
    "        else:\n",
    "            curr_week = [end_date.date().strftime('%Y-%m-%d')]\n",
    "            all_aggr_init.append(curr_week)\n",
    "        \n",
    "        #now we keep only the first and last item of each interval\n",
    "        \n",
    "        all_aggr = []\n",
    "        for interval in all_aggr_init:\n",
    "            interval_new = [interval[0], interval[-1]]\n",
    "            all_aggr.append(interval_new)\n",
    "        \n",
    "        #we now query for each interval\n",
    "        for interval in all_aggr:\n",
    "            query_dict['date'] = interval\n",
    "            query = prepare_sql_query(query_dict)\n",
    "            query_results = make_sql_query(query, database)\n",
    "            query_results_dict[query_dict['date'][0]] = query_results\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        #we have a single date to render for, so nothing to aggregate!\n",
    "        #just in case we check that there is no mismatch between the single day and the\n",
    "        #argument containing specific weekdays restrictions if any\n",
    "        if specific_weekdays == 'on_specific_weekdays':\n",
    "\n",
    "            #we check if the date of the daterange matches the weekday(s) we target\n",
    "            date = pd.Timestamp(query_dict['date'])\n",
    "\n",
    "            if date.dayofweek in weekdays:\n",
    "                query = prepare_sql_query(query_dict)\n",
    "                query_results = make_sql_query(query, database)\n",
    "                query_results_dict[query_dict['date']] = query_results\n",
    "\n",
    "            else:\n",
    "                print(\"The date selected does not match the weekday(s) indicated. \"\n",
    "                    \"Please select either an interval ('time_granularity':'period') \"\n",
    "                    \"or a valid weekday(s) list.\")\n",
    "\n",
    "        else:\n",
    "            query = prepare_sql_query(query_dict)\n",
    "            query_results = make_sql_query(query, database)\n",
    "            query_results_dict[query_dict['date']] = query_results\n",
    "            \n",
    "            \n",
    "    return query_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_animation_query_output(render_animation_dict):\n",
    "    \n",
    "    #We extract the variables we will need from the input dictionary\n",
    "    query_dict = render_animation_dict['query_dict']\n",
    "    query_results_dict = render_animation_dict['query_results_dict']\n",
    "    base_map = render_animation_dict['base_map']\n",
    "    map_type = render_animation_dict['map_type']\n",
    "    shape_dict = render_animation_dict['shape_dict']\n",
    "    df_sf = render_animation_dict['df_sf']\n",
    "    database = render_animation_dict['database']\n",
    "    image_size = render_animation_dict['image_size']\n",
    "    render_single_borough = render_animation_dict['render_single_borough']\n",
    "    min_passenger = render_animation_dict['min_passenger']\n",
    "    max_passenger = render_animation_dict['max_passenger']\n",
    "    aggregate_period = render_animation_dict['aggregate_period']\n",
    "        \n",
    "    if query_dict['filter_query_on_borough'] == False:\n",
    "        #in this case, we may want the base map to be reduced to map_type, but the query\n",
    "        #to be performed on the whole city - thus we want to represent points that may\n",
    "        #not be inside the shape of the reduced base map\n",
    "        projection = render_animation_dict['projection']\n",
    "        converted_shape_dict = convert_shape_boundaries(shape_dict, projection)\n",
    "    \n",
    "    else:\n",
    "        #we isolate the set of zones we want to draw points for in the right coordinate system\n",
    "        converted_shape_dict, projection = get_shape_set_to_draw(map_type, shape_dict, df_sf, image_size)\n",
    "\n",
    "    #we build a dictionary for the details of the rendering of each frame\n",
    "    render_frame_dict = {'database':database, 'min_passenger':min_passenger, \n",
    "                         'max_passenger':max_passenger, 'base_map':base_map, \n",
    "                         'converted_shape_dict':converted_shape_dict,\n",
    "                         'map_type':map_type,'frames':[], 'agg_per':aggregate_period}\n",
    "    \n",
    "    \n",
    "    #we render frames depending on the results of the query and the period inputted\n",
    "    for query_date in query_results_dict:\n",
    "        render_frame_dict['query_date'] = query_date\n",
    "        render_frame_dict['query_results'] = query_results_dict[query_date]\n",
    "        frames = render_all_frames(render_frame_dict)\n",
    "        render_frame_dict['frames'] = frames\n",
    "    \n",
    "\n",
    "    if map_type == 'total':\n",
    "        print('Rendering the results for NYC...')\n",
    "    else:\n",
    "        print('Rendering the results for {}...'.format(map_type))\n",
    "    \n",
    "    #we compile the video from all frames\n",
    "    make_video_animation(frames, image_size, map_type)\n",
    "    \n",
    "\n",
    "    if map_type == 'total':\n",
    "        print('The video for NYC has been rendered')\n",
    "    else:\n",
    "        print('The video for {} has been rendered'.format(map_type))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python script**\n",
    "\n",
    "We can the define the python script that will rely on all functions provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_flow_animation(animation_dict):\n",
    "    #we extract the variables from the input dictionary\n",
    "    shp_path = animation_dict['shp_path']\n",
    "    image_size = animation_dict['image_size']\n",
    "    map_to_render = animation_dict['map_to_render']\n",
    "    render_single_borough = animation_dict['render_single_borough']\n",
    "    title = animation_dict['title']\n",
    "    database = animation_dict['db']\n",
    "    data_table = animation_dict['data_table']\n",
    "    lookup_table = animation_dict['lookup_table']\n",
    "    aggregated_result = animation_dict['aggregated_result']\n",
    "    filter_query_on_borough = animation_dict['filter_query_on_borough']\n",
    "    time_granularity = animation_dict['time_granularity']\n",
    "    period = animation_dict['period']\n",
    "    weekdays = animation_dict['weekdays']\n",
    "    aggregate_period = animation_dict['aggregate_period']\n",
    "    \n",
    "    #First import the shapefile and build the boundaries dictionary\n",
    "    print('Building the base map...')\n",
    "    shp_path = shp_path\n",
    "    sf_nyc = shp.Reader(shp_path)\n",
    "    df_sf = shp_to_df(sf_nyc)\n",
    "    shape_boundaries = process_shape_boundaries(df_sf, sf_nyc)\n",
    "    \n",
    "    #optional fool_proof check\n",
    "    #if filter on borough is not False, then it contains the name of a borouhgh, \n",
    "    #that happens to be the only one we want to use to draw the base map\n",
    "    #so we ignore the input of the user in the map_to_render argument\n",
    "    if filter_query_on_borough !=False:\n",
    "        map_to_render = [filter_query_on_borough]\n",
    "    \n",
    "    #Draw the base map and keep it in a saved variable\n",
    "    base_maps = []\n",
    "    if len(map_to_render) == 1:\n",
    "        map_type = map_to_render[0]\n",
    "        #we want to render on a single map\n",
    "        draw_dict = {'image_size':image_size, 'render_single_borough':render_single_borough, \n",
    "                     'map_type':map_type, 'title':title, \n",
    "                     'shape_dict':shape_boundaries, 'df_sf':df_sf}\n",
    "        base_map, projection = draw_base_map(draw_dict)\n",
    "        base_maps.append((map_type, base_map, projection))\n",
    "    \n",
    "    else:\n",
    "        #we want to render multiple animations at once, for different base maps\n",
    "        for single_map in map_to_render:\n",
    "            map_type = single_map\n",
    "            draw_dict = {'image_size':image_size, 'render_single_borough':render_single_borough,\n",
    "                         'map_type':map_type, 'title':title, \n",
    "                         'shape_dict':shape_boundaries, 'df_sf':df_sf}\n",
    "            base_map, projection = draw_base_map(draw_dict)\n",
    "            base_maps.append((map_type, base_map, projection))\n",
    "    \n",
    "    #we define the render_animation_dict\n",
    "    render_animation_dict = {'time_granularity':time_granularity, 'period':period,  \n",
    "                             'weekdays':weekdays,'filter_query_on_borough':filter_query_on_borough, \n",
    "                             'image_size':image_size,'shape_dict':shape_boundaries, \n",
    "                             'df_sf':df_sf,'database':database, 'data_table':data_table, \n",
    "                             'lookup_table':lookup_table,'aggregated_result':aggregated_result,\n",
    "                             'render_single_borough':render_single_borough,\n",
    "                             'video_title':title, 'aggregate_period':aggregate_period}\n",
    "\n",
    "    #we query the database\n",
    "    print('Querying the dabase...')\n",
    "         \n",
    "    query_dict = build_query_dict(render_animation_dict)\n",
    "    render_animation_dict['query_dict'] = query_dict    \n",
    "    query_results_dict = process_query_arg(render_animation_dict)\n",
    "    \n",
    "    \n",
    "    #we find the min and max passengers for the whole year\n",
    "    min_passenger = 999999999\n",
    "    max_passenger = 0\n",
    "    for query_date in query_results_dict:\n",
    "        temp_min, temp_max = compute_min_max_passengers(query_results_dict[query_date], 2)    \n",
    "        if temp_min < min_passenger:\n",
    "            min_passenger = temp_min\n",
    "        if temp_max > max_passenger:\n",
    "            max_passenger = temp_max\n",
    "    \n",
    "    render_animation_dict['query_results_dict'] = query_results_dict\n",
    "    render_animation_dict['min_passenger'] = min_passenger\n",
    "    render_animation_dict['max_passenger'] = max_passenger\n",
    "    \n",
    "    #we render the animation!\n",
    "    for map_type, base_map, projection in base_maps:\n",
    "        #we add variabled to the render frame dictionary\n",
    "        render_animation_dict['base_map'] = base_map\n",
    "        render_animation_dict['projection'] = projection\n",
    "        render_animation_dict['map_type'] = map_type\n",
    "\n",
    "        render_animation_query_output(render_animation_dict)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses as an input a dictionary, which structure is as follows:\n",
    "\n",
    "animation_dict = {'shp_path':shp_path, 'image_size':(1920,1080), \n",
    "                  'map_to_render':['total', 'Manhattan'],'render_single_borough':False,\n",
    "                  'filter_query_on_borough':False,\n",
    "                  'title':'General flow of passengers in 2018', \n",
    "                  'db':'nyc_taxi_rides', 'data_table':'taxi_rides_2018',\n",
    "                 'lookup_table':'taxi_zone_lookup_table', 'aggregated_result':'count',\n",
    "                 'time_granularity':'period', 'period':['2018-01-01','2018-01-03'],  \n",
    "                 'weekdays':()}\n",
    "                 \n",
    "**Arguments:**\n",
    "- shp_path: the path to the shapefile used to render the base map\n",
    "- image_size: the size of each frame [width, height]\n",
    "- map_to_render: the base map(s) we want animations for. Always provided as a list. If more than one item is in the list, one animation per item will be rendered.\n",
    "- render_single_borough: flag to indicate whether we want to focus on a single borough and render *only* the borough (in this case True), or if we simply want to center and zoom on a borough but still render the rest of the map (in this case False)\n",
    "- filter_query_on_borough: whether we want to execute the query filtering on a borough, or if we want the results for the whole city\n",
    "- title: the title to display in the animation\n",
    "- db: the name of the database to connect to\n",
    "- data_table: the table in which to fetch the data (in our case, the table in which we have the data for 2018)\n",
    "- lookup_table: the taxi zone lookup table, to match a zone id with the name of a borough\n",
    "- aggregated_result: the type of result we want from the query, either avg or count (note that the query results will always be structured 'PULocationID', 'DOLocationID', aggregated_result). \n",
    "- time_granularity: if we want to filter for specific weekdays or we want results for every day in the provided period\n",
    "- period: the time interval to consider for the query. If we want for a single date, start and end date should be inputted the same.\n",
    "- weekdays: the index of the weekday(s) we want data for (0 being Monday, 6 being Sunday). If we want to filter on one or more weekday, time_granularity should be set to 'on_specific_weekdays'. If we we do not want to filter on any weekday, time_granularity should be set to 'period' and the tuple of weekdays left empty (). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Building a python script - part 2 (heat maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for this second rendering option, there is a lot we can reuse from the animation logic. The idea of this second visualisation is to render four heat maps per zone:\n",
    "- incoming flow\n",
    "- outgoing flow\n",
    "- difference in incoming flow between weekdays and weekends\n",
    "- difference in outgoing flow beteen weekdays and weekends\n",
    "\n",
    "Note that we are going to work with average or count of the number of passengers on the period selected by the user. So say we want the map for 2018 with the aggregated_result set to avg, we are going to average the total number of passenger in 2018, splitted per incoming and outgoing flow. If aggregated_result is set to count, we will simply sum the counts over the year.\n",
    "Likewise for the comparison of the weekdays and weekends, we will average the number of passengers of weekdays on the whole period, and subtract to it the number of passengers on weekends. Which means that if we have a positive value, there are more people traveling on weekdays, and if we have a negative value, there are more people traveling on weekends. \n",
    "\n",
    "\n",
    "What we will want to do is:\n",
    "- render a base map: either for the whole city, or solely for the borough in which we have the zone we are focusing on.\n",
    "- make four queries\n",
    "    - average on the period of the number of passengers grouped by *pick up* location ID -> we will look at *outgoing* flow\n",
    "    - average on the period of the number of passengers grouped by *drop off* location ID -> we will look at *incoming* flow\n",
    "    - difference between the average on the period on weekdays and weekends of the number of passengers grouped by *pick up* location ID -> we will look at average difference of *outgoing* flow between weekdays and weekends \n",
    "    - difference between the average on the period on weekdays and weekends of the number of passengers grouped by *drop off* location ID -> we will look at average difference of *incoming* flow between weekdays and weekends \n",
    "- draw the result maps\n",
    "\n",
    "As a result, we are going to render quite a lot of map! (4 maps per zone (one focused on the borough, one focused on the zone), 263 zones at most, so more than 1000 maps!). To make browsing easier, we will make the maps available on a web page with a dropdown list to select and display the desired map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we realise that the query to compute the difference of the average on a given period between the weekdays and weekends numbers of passengers is going to be pushy.\n",
    "In order to speed up calculation time, let's create another table in the database, called passenger_count_2018, that will contain for each day and each link (grouped from origin PULocationID to destination DOLocationID) the total number of passengers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE passenger_count_2018 (\n",
    "\tid INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n",
    "\tpickup_date DATE NULL,\n",
    "    pickup_weekday INTEGER NOT NULL,\n",
    "    passenger_count_per_day FLOAT NULL,\n",
    "\tPULocationID INTEGER NOT NULL,\n",
    "\tDOLocationID INTEGER NOT NULL,\n",
    "\tINDEX pickup_date (pickup_date),\n",
    "\tINDEX pickup_weekday (pickup_weekday),\n",
    "\tFOREIGN KEY (PULocationID) REFERENCES taxi_zone_lookup_table(LocationID),\n",
    "\tFOREIGN KEY (DOLocationID) REFERENCES taxi_zone_lookup_table(LocationID)\n",
    ");\n",
    "\n",
    "INSERT INTO passenger_count_2018 (pickup_date, pickup_weekday, passenger_count_per_day, PULocationID, DOLocationID) \n",
    "SELECT pickup_date, pickup_weekday, COUNT(passenger_count), PULocationID, DOLocationID\n",
    "FROM taxi_rides_2018\n",
    "WHERE pickup_date BETWEEN '2018-01-01 00:00:00' AND '2018-12-31 23:59:59'\n",
    "GROUP BY PULocationID, DOLocationID, pickup_date, pickup_weekday;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are all set with the database, let's define the missing functions (those specific to the rendering oh the heat maps). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just an insight on the rather big query that we make when we want to process the difference between weekdays and weekends. (example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SELECT wd_pu_id pu_id, wd_do_id do_id, wd_aggregated_result - we_aggregated_result diff\n",
    "FROM(SELECT CASE WHEN wd_pu_id IS NULL THEN we_pu_id ELSE wd_pu_id END AS wd_pu_id, \n",
    "\t\t\t\tCASE WHEN wd_do_id IS NULL THEN we_do_id ELSE wd_do_id END AS wd_do_id,\n",
    "\t\t\t\tCASE WHEN wd_aggregated_result IS NULL THEN 0 ELSE wd_aggregated_result END AS wd_aggregated_result,\n",
    "\t\t\t\tCASE WHEN we_pu_id IS NULL THEN wd_pu_id ELSE we_pu_id END AS we_pu_id, \n",
    "\t\t\t\tCASE WHEN we_do_id IS NULL THEN wd_do_id ELSE we_do_id END AS we_do_id,\n",
    "\t\t\t\tCASE WHEN we_aggregated_result IS NULL THEN 0 ELSE we_aggregated_result END AS we_aggregated_result\n",
    "FROM (SELECT *\n",
    "\tFROM (SELECT PULocationID wd_pu_id, DOLocationID wd_do_id, COUNT(passenger_count_per_day) wd_aggregated_result\n",
    "\t\t\tFROM passenger_count_2018\n",
    "\t\t\tWHERE pickup_date BETWEEN '2018-01-01' AND '2018-01-07' AND pickup_weekday IN (0, 1, 2, 3, 4) \n",
    "\t\t\tGROUP BY wd_pu_id, wd_do_id) as weekdays\n",
    "\tLEFT JOIN (SELECT PULocationID we_pu_id, DOLocationID we_do_id, COUNT(passenger_count_per_day) we_aggregated_result\n",
    "\t\t\tFROM passenger_count_2018\n",
    "\t\t\tWHERE pickup_date BETWEEN '2018-01-01' AND '2018-01-07' AND pickup_weekday IN (5, 6) \n",
    "\t\t\tGROUP BY we_pu_id, we_do_id) as weekends\n",
    "\tON weekdays.wd_pu_id = weekends.we_pu_id AND weekdays.wd_do_id = weekends.we_do_id\n",
    "\tUNION \n",
    "    SELECT *\n",
    "\tFROM (SELECT PULocationID wd_pu_id, DOLocationID wd_do_id, COUNT(passenger_count_per_day) wd_aggregated_result\n",
    "\t\t\tFROM passenger_count_2018\n",
    "\t\t\tWHERE pickup_date BETWEEN '2018-01-01' AND '2018-01-07' AND pickup_weekday IN (0, 1, 2, 3, 4) \n",
    "\t\t\tGROUP BY wd_pu_id, wd_do_id) as weekdays\n",
    "\tRIGHT JOIN (SELECT PULocationID we_pu_id, DOLocationID we_do_id, COUNT(passenger_count_per_day) we_aggregated_result\n",
    "\t\t\tFROM passenger_count_2018\n",
    "\t\t\tWHERE pickup_date BETWEEN '2018-01-01' AND '2018-01-07' AND pickup_weekday IN (5, 6) \n",
    "\t\t\tGROUP BY we_pu_id, we_do_id) as weekends\n",
    "\tON weekdays.wd_pu_id = weekends.we_pu_id AND weekdays.wd_do_id = weekends.we_do_id) as table_1) as table_2;\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table we want to query is an intermediate, pre-processed table, that already contains the count of passengers per link per day. The idea of using preprocessed data, as well as having both the date and the weekday used as indexes, is to speed up the calculation.\n",
    "And indeed, we need it when it comes to compute the difference in the number of passengers between weekdays and weekends, because we need to join several tables.\n",
    "\n",
    "The query works as follow:\n",
    "- we left join a table extracting only weekdays count of people with a table extracting only weekends count of people. With this table, we might have rows from the weekends table that contains only NULL values, so we will want to replace them with the PULocationID and DOLocationID of the weekdays table, and 0 as a count of people.\n",
    "- we right join a table extracting only weekdays count of people with a table extracting only weekends count of people. With this table, we might have rows from the weekdays table that contains only NULL values, so we will want to replace them with the PULocationID and DOLocationID of the weekends table, and 0 as a count of people.\n",
    "- we union these two tables, and use CASE statements to replace the NULL values we gathered from the joins. We then have the PULocationID and DOLocationID of both the weekdays and weekends that are the same, and some 0 values for the counts of people.\n",
    "- we select only one PULocationID column, one DOLocationID column, and compute the difference in the counts of people.\n",
    "\n",
    "If needed, we add a statement to join the lookup table in order to filter per borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_heat_map_sql_query(query_dict):\n",
    "    \n",
    "    #We extract the variables we will need from the input dictionary\n",
    "    data_table = query_dict['data_table']\n",
    "    lookup_table = query_dict['lookup_table']\n",
    "    aggregated_result = query_dict['aggregated_result']\n",
    "    date = query_dict['date']\n",
    "    filter_query_on_borough = query_dict['filter_query_on_borough']\n",
    "    weekdays_vs_weekends = query_dict['specific_weekdays']\n",
    "    \n",
    "    #first we synthesise what we want to fetch\n",
    "    if aggregated_result == 'count':\n",
    "        #we will want to return the sum of count on the period\n",
    "        aggregated_result = 'COUNT(passenger_count_per_day)'\n",
    "    elif aggregated_result == 'avg':\n",
    "        #we will want to return the average of count on the period\n",
    "        aggregated_result = 'AVG(passenger_count_per_day)'\n",
    "        \n",
    "    #we prepare the period statements    \n",
    "    if type(date) == str:\n",
    "        #in this case, we want the result on a single day\n",
    "        date_statement = (\"pickup_date = '{}'\").format(date)\n",
    "    else:\n",
    "        #we provided a time interval we want the average of the aggregated_result on the\n",
    "        #period\n",
    "        start_date = date[0]\n",
    "        end_date = date[1]\n",
    "        date_statement = (\"pickup_date BETWEEN '{}' AND '{}'\").format(start_date, end_date)\n",
    "    \n",
    "    \n",
    "    #we build the query\n",
    "    if weekdays_vs_weekends == 'weekdays_vs_weekends':\n",
    "        #in this situation we want to query 'separately' the values in weekdays and weekends\n",
    "        #and make a difference on the average of the aggregated_result on the period\n",
    "        date_statement_weekdays = ( \"pickup_date BETWEEN '{}' AND '{}' AND pickup_weekday IN (0, 1, 2, 3, 4)\".format(start_date, end_date))\n",
    "        date_statement_weekends = (\"pickup_date BETWEEN '{}' AND '{}' AND pickup_weekday IN (5, 6)\".format(start_date, end_date))\n",
    "        \n",
    "        \n",
    "        #Case 1: we want to compare weekdays and weekends flow for a specific borough\n",
    "        if filter_query_on_borough != False:\n",
    "            query = (\"SELECT pu_id, do_id, diff \\\n",
    "                    FROM (SELECT wd_pu_id pu_id, wd_do_id do_id, wd_aggregated_result - we_aggregated_result diff\\\n",
    "                        FROM(SELECT CASE WHEN wd_pu_id IS NULL THEN we_pu_id ELSE wd_pu_id END AS wd_pu_id, \\\n",
    "                                    CASE WHEN wd_do_id IS NULL THEN we_do_id ELSE wd_do_id END AS wd_do_id,\\\n",
    "                                    CASE WHEN wd_aggregated_result IS NULL THEN 0 ELSE wd_aggregated_result END AS wd_aggregated_result,\\\n",
    "                                    CASE WHEN we_pu_id IS NULL THEN wd_pu_id ELSE we_pu_id END AS we_pu_id, \\\n",
    "                                    CASE WHEN we_do_id IS NULL THEN wd_do_id ELSE we_do_id END AS we_do_id,\\\n",
    "                                    CASE WHEN we_aggregated_result IS NULL THEN 0 ELSE we_aggregated_result END AS we_aggregated_result\\\n",
    "                        FROM (SELECT *\\\n",
    "                                FROM (SELECT PULocationID wd_pu_id, DOLocationID wd_do_id, {0} wd_aggregated_result\\\n",
    "                                    FROM {1}\\\n",
    "                                    WHERE {2} \\\n",
    "                                    GROUP BY wd_pu_id, wd_do_id) as weekdays\\\n",
    "                                LEFT JOIN (SELECT PULocationID we_pu_id, DOLocationID we_do_id, {0} we_aggregated_result\\\n",
    "                                        FROM {1}\\\n",
    "                                        WHERE {3} \\\n",
    "                                        GROUP BY we_pu_id, we_do_id) as weekends\\\n",
    "                                ON weekdays.wd_pu_id = weekends.we_pu_id \\\n",
    "                                    AND weekdays.wd_do_id = weekends.we_do_id\\\n",
    "                            UNION \\\n",
    "                                SELECT *\\\n",
    "                                FROM (SELECT PULocationID wd_pu_id, DOLocationID wd_do_id, {0} wd_aggregated_result\\\n",
    "                                        FROM {1}\\\n",
    "                                        WHERE {2} \\\n",
    "                                        GROUP BY wd_pu_id, wd_do_id) as weekdays\\\n",
    "                                RIGHT JOIN (SELECT PULocationID we_pu_id, DOLocationID we_do_id, {0} we_aggregated_result\\\n",
    "                                            FROM {1}\\\n",
    "                                            WHERE {3} \\\n",
    "                                            GROUP BY we_pu_id, we_do_id) as weekends\\\n",
    "                                ON weekdays.wd_pu_id = weekends.we_pu_id \\\n",
    "                                 AND weekdays.wd_do_id = weekends.we_do_id) as tab_1) as tab_2\\\n",
    "                    JOIN {4} lookup_pu\\\n",
    "                    ON lookup_pu.LocationID = tab_2.pu_id \\\n",
    "                    JOIN {4} lookup_do \\\n",
    "                    ON lookup_do.LocationID = tab_2.do_id \\\n",
    "                    WHERE lookup_pu.borough_name = '{5}' AND lookup_do.borough_name = '{5}'\\\n",
    "                    GROUP BY pu_id, do_id, diff;\".format(aggregated_result, data_table, \n",
    "                                                         date_statement_weekdays, \n",
    "                                                         date_statement_weekends, \n",
    "                                                         lookup_table,\n",
    "                                                         filter_query_on_borough))\n",
    "            \n",
    "            \n",
    "        #Case 2: we want to compare weekdays and weekends flow for the whole city        \n",
    "        else:\n",
    "            query = (\"SELECT wd_pu_id pu_id, wd_do_id do_id, wd_aggregated_result - we_aggregated_result diff\\\n",
    "                    FROM(SELECT CASE WHEN wd_pu_id IS NULL THEN we_pu_id ELSE wd_pu_id END AS wd_pu_id, \\\n",
    "                                CASE WHEN wd_do_id IS NULL THEN we_do_id ELSE wd_do_id END AS wd_do_id,\\\n",
    "                                CASE WHEN wd_aggregated_result IS NULL THEN 0 ELSE wd_aggregated_result END AS wd_aggregated_result,\\\n",
    "                                CASE WHEN we_pu_id IS NULL THEN wd_pu_id ELSE we_pu_id END AS we_pu_id, \\\n",
    "                                CASE WHEN we_do_id IS NULL THEN wd_do_id ELSE we_do_id END AS we_do_id,\\\n",
    "                                CASE WHEN we_aggregated_result IS NULL THEN 0 ELSE we_aggregated_result END AS we_aggregated_result\\\n",
    "                    FROM (SELECT *\\\n",
    "                            FROM (SELECT PULocationID wd_pu_id, DOLocationID wd_do_id, {0} wd_aggregated_result\\\n",
    "                                FROM {1}\\\n",
    "                                WHERE {2} \\\n",
    "                                GROUP BY wd_pu_id, wd_do_id) as weekdays\\\n",
    "                            LEFT JOIN (SELECT PULocationID we_pu_id, DOLocationID we_do_id, {0} we_aggregated_result\\\n",
    "                                    FROM {1}\\\n",
    "                                    WHERE {3} \\\n",
    "                                    GROUP BY we_pu_id, we_do_id) as weekends\\\n",
    "                            ON weekdays.wd_pu_id = weekends.we_pu_id \\\n",
    "                                AND weekdays.wd_do_id = weekends.we_do_id\\\n",
    "                        UNION \\\n",
    "                            SELECT *\\\n",
    "                            FROM (SELECT PULocationID wd_pu_id, DOLocationID wd_do_id, {0} wd_aggregated_result\\\n",
    "                                    FROM {1}\\\n",
    "                                    WHERE {2} \\\n",
    "                                    GROUP BY wd_pu_id, wd_do_id) as weekdays\\\n",
    "                            RIGHT JOIN (SELECT PULocationID we_pu_id, DOLocationID we_do_id, {0} we_aggregated_result\\\n",
    "                                        FROM {1}\\\n",
    "                                        WHERE {3} \\\n",
    "                                        GROUP BY we_pu_id, we_do_id) as weekends\\\n",
    "                            ON weekdays.wd_pu_id = weekends.we_pu_id \\\n",
    "                             AND weekdays.wd_do_id = weekends.we_do_id) as tab_1) as tab_2;\".format(aggregated_result, data_table, \n",
    "                                                                                                 date_statement_weekdays, \n",
    "                                                                                                 date_statement_weekends))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        #Case 3: we want the total average/count on the period for a specific borough\n",
    "        if filter_query_on_borough != False:\n",
    "            query = (\"SELECT pu_id, do_id, {0} aggregated_result \\\n",
    "                    FROM \\\n",
    "                         (SELECT PULocationID pu_id, DOLocationID do_id, \\\n",
    "                                 passenger_count_per_day\\\n",
    "                        FROM {1}\\\n",
    "                        WHERE {2} \\\n",
    "                        GROUP BY pu_id, do_id) as tab_1 \\\n",
    "                    JOIN {3} lookup_pu\\\n",
    "                    ON lookup_pu.LocationID = tab_1.pu_id \\\n",
    "                    JOIN {3} lookup_do \\\n",
    "                    ON lookup_do.LocationID = tab_1.do_id \\\n",
    "                    WHERE lookup_pu.borough_name = '{4}' AND lookup_do.borough_name = '{4}'\\\n",
    "                    GROUP BY pu_id, do_id\".format(aggregated_result, data_table, date_statement,\n",
    "                                                   lookup_table, filter_query_on_borough))\n",
    "        \n",
    "        #Case 4: we want the total average/count on the period for the whole city \n",
    "        else:\n",
    "            query = (\"SELECT PULocationID pu_id, DOLocationID do_id, {0} aggregated_result\\\n",
    "                    FROM {1}\\\n",
    "                    WHERE {2} \\\n",
    "                    GROUP BY pu_id, do_id\".format(aggregated_result, data_table, date_statement))\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_heat_map_query_results(query_results):\n",
    "    \n",
    "    incoming_flow = {}\n",
    "    outgoing_flow = {}\n",
    "    \n",
    "    #then we build a dictionary of outgoing traffic i.e each zone_id used \n",
    "    #as a key in the dict will have a count of people going to another zone\n",
    "    for itinerary in query_results:\n",
    "        origin_id = itinerary[0]\n",
    "        destination_id = itinerary[1]\n",
    "        weight = itinerary[2]\n",
    "        if origin_id not in outgoing_flow:\n",
    "            outgoing_flow[origin_id] = []\n",
    "        outgoing_flow[origin_id].append((destination_id, weight))\n",
    "\n",
    "    #we finally do the same but with the incoming trafic i.e each zone_id used \n",
    "    #as a key in the dict will have a count of people coming from another zone\n",
    "    for itinerary in query_results:\n",
    "        origin_id = itinerary[0]\n",
    "        destination_id = itinerary[1]\n",
    "        weight = itinerary[2]\n",
    "        if destination_id not in incoming_flow:\n",
    "            incoming_flow[destination_id] = []\n",
    "        incoming_flow[destination_id].append((origin_id, weight))\n",
    "    \n",
    "    return outgoing_flow, incoming_flow\n",
    "        \n",
    "\n",
    "def find_names(zone_id, df_sf):\n",
    "    \n",
    "    zone_name = df_sf[df_sf.index==zone_id]['zone'].item()\n",
    "    borough_name = df_sf[df_sf.index==zone_id]['borough'].item()\n",
    "    \n",
    "    return zone_name, borough_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_color(weight, min_passenger, max_passenger):\n",
    "    \n",
    "    #we use a color palette that spans between two very different colors, the idea\n",
    "    #being to be able to distinguish positive from negative values\n",
    "    \n",
    "    max_pos_colour = (100, 100, 255)#shade of red\n",
    "    min_pos_colour = (40, 40, 100)#shade of red\n",
    "    min_neg_colour = (0, 0, 0)#shade of blue\n",
    "    max_neg_colour = (210, 150, 90)#shade of blue\n",
    "    \n",
    "    if weight == 0:\n",
    "        color = [40, 40, 40]#grey\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if min_passenger == max_passenger:\n",
    "            #in this case we have basically one color to represent only\n",
    "            if weight > 0:\n",
    "                color = max_pos_colour\n",
    "\n",
    "            else:\n",
    "                color = max_neg_colour\n",
    "                \n",
    "        elif min_passenger >= 0 and max_passenger > 0:\n",
    "            #in this case we draw everything in shades of red\n",
    "            weight_norm = weight/max_passenger\n",
    "            blue_index = (max_pos_colour[0]-min_pos_colour[0])*weight_norm + min_pos_colour[0]\n",
    "            green_index = (max_pos_colour[1]-min_pos_colour[1])*weight_norm + min_pos_colour[1]\n",
    "            red_index = (max_pos_colour[2]-min_pos_colour[2])*weight_norm + min_pos_colour[2]\n",
    "            color = (blue_index, green_index, red_index)\n",
    "        \n",
    "        elif min_passenger < 0 and max_passenger <= 0:\n",
    "            #in this case we draw everything in shades of blue\n",
    "            weight_norm = weight/min_passenger\n",
    "            blue_index = (max_neg_colour[0]-min_neg_colour[0])*weight_norm + min_neg_colour[0]\n",
    "            green_index = (max_neg_colour[1]-min_neg_colour[1])*weight_norm + min_neg_colour[1]\n",
    "            red_index = (max_neg_colour[2]-min_neg_colour[2])*weight_norm + min_neg_colour[2]\n",
    "            color = (blue_index, green_index, red_index)        \n",
    "            \n",
    "        else:\n",
    "            #in this case the color depends on the sign of the weight\n",
    "            #we call this function recursively\n",
    "            if weight > 0:\n",
    "                color = compute_color(weight, 0, max_passenger)\n",
    "\n",
    "            else:\n",
    "                color = compute_color(weight, min_passenger, 0)\n",
    "            \n",
    "    \n",
    "    return color\n",
    "\n",
    "\n",
    "def render_map(render_map_dict):\n",
    "    \n",
    "    #first we extract the arguments we are going to need\n",
    "    map_to_render = render_map_dict['map_to_render']\n",
    "    zone_id = render_map_dict['zone_id']\n",
    "    trips_list = render_map_dict['trips_list']\n",
    "    draw_dict = render_map_dict['draw_dict']\n",
    "    shape_dict = draw_dict['shape_dict']\n",
    "    draw_dict['map_type']= map_to_render\n",
    "    min_passenger = render_map_dict['min_passenger']\n",
    "    max_passenger = render_map_dict['max_passenger']\n",
    "        \n",
    "    base_map, projection = draw_base_map(draw_dict)\n",
    "        \n",
    "    #we obtain the converted_shape_dict we want to use to draw the heat map\n",
    "    converted_shape_dict = convert_shape_boundaries(shape_dict, projection)\n",
    "\n",
    "    #we keep track of how many colors we use to plot the legend afterwards\n",
    "    colors = []\n",
    "    for linked_zone in trips_list:\n",
    "        id_shape_to_color = linked_zone[0]\n",
    "        if id_shape_to_color != zone_id:\n",
    "            weight = linked_zone[1]\n",
    "            linked_shape = converted_shape_dict[id_shape_to_color]\n",
    "            linked_points = linked_shape['points']\n",
    "            pts = np.array(linked_points, np.int32)\n",
    "            linked_color = compute_color(weight, min_passenger, max_passenger)\n",
    "            if linked_color not in colors:\n",
    "                colors.append(linked_color)\n",
    "            cv2.fillPoly(base_map, [pts], linked_color)\n",
    "            cv2.polylines(base_map, [pts], True, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    #we highlight the focused shape\n",
    "    target_shape = converted_shape_dict[zone_id]\n",
    "    target_points = target_shape['points']\n",
    "    pts = np.array(target_points, np.int32)\n",
    "    target_color = [95, 240, 255]\n",
    "    cv2.polylines(base_map, [pts], True, target_color, 3, cv2.LINE_AA)\n",
    "    \n",
    "    return base_map, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scale_legend(map_image, font, min_pass, max_pass, colors):\n",
    "    #we dynamically print a legend using a fixed step between two colors plotted\n",
    "    \n",
    "    k = 0\n",
    "    top_bar_x = 30\n",
    "    top_bar_y = 440\n",
    "    \n",
    "    #we add a legend for no passengers traveling\n",
    "    cv2.rectangle(map_image,(top_bar_x, top_bar_y),(top_bar_x+40, top_bar_y + 20),(255, 255, 255),1)\n",
    "    cv2.putText(map_image, 'No flow of people', (top_bar_x + 70, top_bar_y + 15), font, 0.7, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    top_bar_y = top_bar_y + 22   \n",
    "        \n",
    "    #we prepare the ground to plot a dynamic legend for the colors\n",
    "    if len(colors) < 8:\n",
    "        scale_step = len(colors)\n",
    "    else:\n",
    "        scale_step = 8\n",
    "    \n",
    "    levels = []\n",
    "    while k < scale_step:\n",
    "        if scale_step > 1:\n",
    "            level = max_pass + (min_pass - max_pass) * k/(scale_step-1)\n",
    "        else:\n",
    "            level = max_pass\n",
    "        levels.append(level)\n",
    "        k+=1\n",
    "    \n",
    "    #we check if there are negative and positive values to represent and if we already\n",
    "    #have a 0 to represent ; if not, we will add it to the list of steps to plot\n",
    "    neg_value_count = 0\n",
    "    pos_value_count = 0\n",
    "    zero_count = 0\n",
    "    for level in levels:\n",
    "        if level < 0:\n",
    "            neg_value_count+= 1\n",
    "        elif level == 0:\n",
    "            zero_count+=1\n",
    "        else:\n",
    "            pos_value_count+=1\n",
    "    \n",
    "    if zero_count == 0:\n",
    "        if neg_value_count > 0 and pos_value_count> 0:\n",
    "            levels.append(0)\n",
    "    \n",
    "    #we plot dynamically the legend\n",
    "    levels.sort()\n",
    "    for level in levels:   \n",
    "        color = compute_color(level, min_pass, max_pass)\n",
    "        level = \"{0:.2f}\".format(level)\n",
    "        cv2.rectangle(map_image,(top_bar_x, top_bar_y),(top_bar_x+40, top_bar_y + 20),color,-1)\n",
    "        if float(level) == 0 or abs(float(level)) == 1:\n",
    "            cv2.putText(map_image, '{} passenger'.format(level), (top_bar_x + 70, top_bar_y + 15), font, 0.7, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(map_image, '{} passengers'.format(level), (top_bar_x + 70, top_bar_y + 15), font, 0.7, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "        top_bar_y = top_bar_y + 20\n",
    "    \n",
    "      \n",
    "\n",
    "def display_specific_text_heat_map(map_image, time_granularity, zone_info, min_pass, max_pass, colors):\n",
    "    \n",
    "    #note that these position are based on an image size of [1920, 1080]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    if time_granularity == 'period':\n",
    "        time_granularity_1 = 'Flow over the whole year'\n",
    "        time_granularity_2 = \"\"\n",
    "    else:\n",
    "        time_granularity_1 = \"Difference between weekdays\"\n",
    "        time_granularity_2 = \"and weekends flow\"\n",
    "    \n",
    "    #display the main title\n",
    "    cv2.putText(map_image, time_granularity_1, (30, 150), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    cv2.putText(map_image, time_granularity_2, (30, 180), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    \n",
    "    #display the zone id and name\n",
    "    cv2.putText(map_image, '{} - '.format(zone_info[0]), (30, 240), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    cv2.putText(map_image, '{}'.format(zone_info[1]), (170, 240), font, 1.3, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    \n",
    "    #displays the legend of the colour code\n",
    "    cv2.putText(map_image, 'Legend', (30,320), font, 0.9, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    cv2.rectangle(map_image,(30,340),(70,360),(95, 240, 255),3)\n",
    "    cv2.putText(map_image, 'Target zone', (100, 360), font, 0.7, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    cv2.putText(map_image, 'Average number of passengers', (30, 410), font, 0.7, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    cv2.putText(map_image, '* A negative value means more flow on weekends', (30, 430), font, 0.5, (221, 221, 221), 1, cv2.LINE_AA)\n",
    "    \n",
    "    display_scale_legend(map_image, font, min_pass, max_pass, colors)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_heat_map_query_output(render_heat_map_dict):\n",
    "    \n",
    "    draw_dict = render_heat_map_dict['draw_dict']\n",
    "    flow_dict = render_heat_map_dict['flow_dict']\n",
    "    flow_dir = render_heat_map_dict['flow_dir']\n",
    "    time_granularity = render_heat_map_dict['time_granularity']\n",
    "    df_sf = draw_dict['df_sf']\n",
    "    \n",
    "    \n",
    "    for zone_id in flow_dict:\n",
    "        #we ensure the ids are in the write 'system'\n",
    "        trips_list = flow_dict[zone_id]\n",
    "        i = 0\n",
    "        for trip in trips_list:\n",
    "            dest_id = convert_id_shape(trip[0])\n",
    "            trips_list[i] = (dest_id, trip[1])\n",
    "            i+=1\n",
    "        zone_id = convert_id_shape(zone_id)\n",
    "        \n",
    "        \n",
    "        #first let's figure out in which borough it is, and which name it has\n",
    "        zone_name, borough_name = find_names(zone_id, df_sf)\n",
    "        \n",
    "        #we want to render the base map on the whole NYC map, as well as on a borough\n",
    "        #zoomed map\n",
    "        \n",
    "        #Let's build the file names\n",
    "        zone_id_lookup = convert_id_shape(zone_id, inverse=True)\n",
    "        if time_granularity == 'weekdays_vs_weekends':\n",
    "            nyc_file_name = 'NYC_{}_{}_{}_2018_diff_WD_WE'.format(zone_id_lookup, zone_name, flow_dir)\n",
    "            borough_file_name = '{}_{}_{}_{}_2018_diff_WD_WE'.format(borough_name, zone_id_lookup, \n",
    "                                                                     zone_name, flow_dir)\n",
    "        else:\n",
    "            nyc_file_name = 'NYC_{}_{}_{}_2018'.format(zone_id_lookup, zone_name, flow_dir)\n",
    "            borough_file_name = '{}_{}_{}_{}_2018'.format(borough_name,zone_id_lookup, \n",
    "                                                          zone_name, flow_dir)\n",
    "        \n",
    "        zone_info = [zone_id_lookup, zone_name]\n",
    "        \n",
    "        #we get the min and max number of passengers and color the linked zones\n",
    "        min_passenger, max_passenger = compute_min_max_passengers(trips_list, 1)\n",
    "    \n",
    "        #Render results on the NYC map\n",
    "        render_map_dict_NYC = {'map_to_render':'total', 'zone_id': zone_id, \n",
    "                               'draw_dict':draw_dict, 'min_passenger':min_passenger, \n",
    "                               'max_passenger':max_passenger, 'trips_list':trips_list}\n",
    "        \n",
    "        nyc_map, nyc_colors = render_map(render_map_dict_NYC)\n",
    "        \n",
    "        #display the legend\n",
    "        display_specific_text_heat_map(nyc_map, time_granularity, zone_info, \n",
    "                                       min_passenger, max_passenger, nyc_colors)\n",
    "\n",
    "        #save the image\n",
    "        cv2.imwrite(('{}.png').format(nyc_file_name),nyc_map)\n",
    "        \n",
    "    \n",
    "\n",
    "        #Render results on the borough map\n",
    "        render_map_dict_borough = {'map_to_render':borough_name, 'zone_id': zone_id, \n",
    "                                   'draw_dict':draw_dict, 'min_passenger':min_passenger, \n",
    "                                   'max_passenger':max_passenger, 'trips_list':trips_list}\n",
    "        \n",
    "        borough_map, borough_colors = render_map(render_map_dict_borough)\n",
    "        \n",
    "        #display the legend\n",
    "        display_specific_text_heat_map(borough_map, time_granularity, zone_info, \n",
    "                                       min_passenger, max_passenger, borough_colors)\n",
    "        \n",
    "\n",
    "        #save the image\n",
    "        cv2.imwrite(('{}.png').format(borough_file_name),borough_map)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heat_map(heat_map_dict):\n",
    "    \n",
    "    #we extract the variables from the input dictionary\n",
    "    shp_path = heat_map_dict['shp_path']\n",
    "    image_size = heat_map_dict['image_size']\n",
    "    render_single_borough = heat_map_dict['render_single_borough']\n",
    "    title = heat_map_dict['title']\n",
    "    database = heat_map_dict['db']\n",
    "    data_table = heat_map_dict['data_table']\n",
    "    lookup_table = heat_map_dict['lookup_table']\n",
    "    aggregated_result = heat_map_dict['aggregated_result']\n",
    "    filter_query_on_borough = heat_map_dict['filter_query_on_borough']\n",
    "    period = heat_map_dict['period']\n",
    "    \n",
    "    if heat_map_dict['weekdays_vs_weekends']==True:\n",
    "        time_granularity = 'weekdays_vs_weekends'\n",
    "    else:\n",
    "        time_granularity = 'period'\n",
    "    \n",
    "    \n",
    "    #Import the shapefile and build the boundaries dictionary\n",
    "    shp_path = shp_path\n",
    "    sf_nyc = shp.Reader(shp_path)\n",
    "    df_sf = shp_to_df(sf_nyc)\n",
    "    shape_boundaries = process_shape_boundaries(df_sf, sf_nyc)\n",
    "    \n",
    "    #we define the render_heat_map_dict    \n",
    "    render_heat_map_dict = {'time_granularity':time_granularity, 'period':period,  \n",
    "                             'image_size':image_size,'data_table':data_table, \n",
    "                             'lookup_table':lookup_table,'aggregated_result':aggregated_result,\n",
    "                             'title':title, 'filter_query_on_borough':filter_query_on_borough}\n",
    "    \n",
    "    #we build the query statement and execute it on the database\n",
    "    print('Querying the dabase...')\n",
    "    query_dict = build_query_dict(render_heat_map_dict)\n",
    "    \n",
    "    if query_dict['date'] == 'loop_through_period':\n",
    "        #if we have the flag loop_through_period in the query dict, it means the period\n",
    "        #set for the query is multiple dates, therefore we want the query to return an\n",
    "        #average on a time interval, and not on a single date\n",
    "        period = render_heat_map_dict['period']\n",
    "        daterange = pd.date_range(period[0],period[1])\n",
    "        query_dict['date'] = period\n",
    "    \n",
    "    query = prepare_heat_map_sql_query(query_dict)\n",
    "    query_results = make_sql_query(query, database)\n",
    "    \n",
    "    \n",
    "    #we process the query results\n",
    "    outgoing_flow, incoming_flow = process_heat_map_query_results(query_results)\n",
    "\n",
    "    draw_dict = {'image_size':image_size, 'render_single_borough':render_single_borough, \n",
    "             'title':title, 'shape_dict':shape_boundaries, 'df_sf':df_sf}\n",
    "    \n",
    "    print('Building the outgoing maps...')\n",
    "    #we build the maps for the outgoing flow\n",
    "    render_heat_map_dict_out = {'draw_dict':draw_dict, 'flow_dict':outgoing_flow, \n",
    "                            'flow_dir': 'out','time_granularity':time_granularity}\n",
    "    \n",
    "    render_heat_map_query_output(render_heat_map_dict_out)  \n",
    "    \n",
    "    print('Building the incoming maps...')\n",
    "    #we build the maps for the incoming flow\n",
    "    render_heat_map_dict_in = {'draw_dict':draw_dict, 'flow_dict':incoming_flow, \n",
    "                            'flow_dir': 'in','time_granularity':time_granularity}\n",
    "    \n",
    "    render_heat_map_query_output(render_heat_map_dict_in) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - render the animations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to generate the animations. All the animations generated here are saved with the notebook. \n",
    "\n",
    "In general, the common arguments we are going to pass to the function are the following:\n",
    "- **shp_path** (same shapefile no matter what animation we are rendering)\n",
    "- **image_size**: [1920, 1080]\n",
    "- **db**: the nyc_taxi_rides database\n",
    "- **data_table**: the taxi_rides_2018 table\n",
    "- **lookup_table**: the taxi_zone_lookup_table table\n",
    "- **aggregated_result**: count\n",
    "- **'aggregate_period'**:whether we want the results to be shown for each day, or aggregated per week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Animation 1 - over the year 2018**\n",
    "\n",
    "This animation is actually composed of a few videos: the flow of passengers everyday in 2018, for the whole city as well as one video per borough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify our specific arguments:\n",
    "- **map_to_render**: for the whole year, we will want to look at the whole city and each borough individually, so we set this to ['total', 'Manhattan', 'Bronx', 'Queens', 'Staten Island', 'Brooklyn']\n",
    "- **render_single_borough**: let's set this to False, so we just focus and zoom on to the borough when rendering them separately\n",
    "- **filter_query_on_borough**: set to False so we make one query per day that we can reuse to render each animation individually\n",
    "- **title**: Let's stay simple, with 'General flow of passengers in 2018'. The name of the borough will be appended automatically to the title when rendering the borough-focused animation\n",
    "- **time_granularity**: we don't want a filter on specific weekdays, so we set this as 'period'\n",
    "- period: we want the whole year, so ['2018-01-01','2018-12-31']!\n",
    "- **weekdays**: we want for the whole year, so let's leave this as an empty array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_path = \"/Users/acoullandreau/Desktop/Taxi_rides_DS/taxi_zones/taxi_zones.shp\"\n",
    "\n",
    "animation_dict_2018 = {'shp_path':shp_path, 'image_size':(1920,1080), \n",
    "                       'map_to_render':['total', 'Manhattan', 'Bronx', 'Queens', 'Staten Island', 'Brooklyn'],\n",
    "                       'render_single_borough':False,\n",
    "                       'filter_query_on_borough':False,\n",
    "                       'title':'General flow of passengers in 2018', \n",
    "                       'db':'nyc_taxi_rides', 'data_table':'taxi_rides_2018',\n",
    "                       'lookup_table':'taxi_zone_lookup_table', 'aggregated_result':'count',\n",
    "                       'time_granularity':'period', 'period':['2018-01-01','2018-12-31'],  \n",
    "                       'weekdays':(), 'aggregate_period':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the base map...\n",
      "Querying the dabase...\n",
      "Rendering the results for NYC...\n",
      "The video for NYC has been rendered\n",
      "Rendering the results for Manhattan...\n",
      "The video for Manhattan has been rendered\n",
      "Rendering the results for Bronx...\n",
      "The video for Bronx has been rendered\n",
      "Rendering the results for Queens...\n",
      "The video for Queens has been rendered\n",
      "Rendering the results for Staten Island...\n",
      "The video for Staten Island has been rendered\n",
      "Rendering the results for Brooklyn...\n",
      "The video for Brooklyn has been rendered\n"
     ]
    }
   ],
   "source": [
    "make_flow_animation(animation_dict_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Animation 2 - over the year 2018, just weekdays**\n",
    "\n",
    "Same logic, let's create several videos per borough, but this time we want to see only weekdays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify our specific arguments:\n",
    "- **map_to_render**: for the whole year, we will want to look at the whole city and each borough individually, so we set this to ['total', 'Manhattan', 'Bronx', 'Queens', 'Staten Island', 'Brooklyn']\n",
    "- **render_single_borough**: let's set this to False, so we just focus and zoom on to the borough when rendering them separately\n",
    "- **filter_query_on_borough**: set to False so we make one query per day that we can reuse to render each animation individually\n",
    "- **title**: Let's stay simple, with 'General flow of passengers on weekdays in 2018'. The name of the borough will be appended automatically to the title when rendering the borough-focused animation\n",
    "- **time_granularity**: we want a filter on specific weekdays, so we set this as 'on_specific_weekdays'\n",
    "- **period**: we want the whole year, so ['2018-01-01','2018-12-31']!\n",
    "- **weekdays**: we want only for week days, so (0, 1, 2, 3, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation_dict_weekdays_2018 = {'shp_path':shp_path, 'image_size':(1920,1080), \n",
    "                                'map_to_render':['total', 'Manhattan', 'Bronx', 'Queens', 'Staten Island', 'Brooklyn'],\n",
    "                                'render_single_borough':False,\n",
    "                                'filter_query_on_borough':False,\n",
    "                                'title':'General flow of passengers on weekdays in 2018', \n",
    "                                'db':'nyc_taxi_rides', 'data_table':'taxi_rides_2018',\n",
    "                                'lookup_table':'taxi_zone_lookup_table', \n",
    "                                'aggregated_result':'count',\n",
    "                                'time_granularity':'on_specific_weekdays', \n",
    "                                'period':['2018-01-01','2018-12-31'],  \n",
    "                                'weekdays':(0, 1, 2, 3, 4), 'aggregate_period':True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the base map...\n",
      "Querying the dabase...\n",
      "Rendering the results for NYC...\n",
      "The video for NYC has been rendered\n",
      "Rendering the results for Manhattan...\n",
      "The video for Manhattan has been rendered\n",
      "Rendering the results for Bronx...\n",
      "The video for Bronx has been rendered\n",
      "Rendering the results for Queens...\n",
      "The video for Queens has been rendered\n",
      "Rendering the results for Staten Island...\n",
      "The video for Staten Island has been rendered\n",
      "Rendering the results for Brooklyn...\n",
      "The video for Brooklyn has been rendered\n"
     ]
    }
   ],
   "source": [
    "make_flow_animation(animation_dict_weekdays_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Animation 3 - over the year 2018, just weekends**\n",
    "\n",
    "Same logic, let's create several videos per borough, but this time we want to see only weekends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify our specific arguments:\n",
    "- **map_to_render**: for the whole year, we will want to look at the whole city and each borough individually, so we set this to ['total', 'Manhattan', 'Bronx', 'Queens', 'Staten Island', 'Brooklyn']\n",
    "- **render_single_borough**: let's set this to False, so we just focus and zoom on to the borough when rendering them separately\n",
    "- **filter_query_on_borough**: set to False so we make one query per day that we can reuse to render each animation individually\n",
    "- **title**: Let's stay simple, with 'General flow of passengers on weekends in 2018'. The name of the borough will be appended automatically to the title when rendering the borough-focused animation\n",
    "- **time_granularity**: we want a filter on specific weekdays, so we set this as 'on_specific_weekdays'\n",
    "- **period**: we want the whole year, so ['2018-01-01','2018-12-31']!\n",
    "- **weekdays**: we want only for weekend days, so (5, 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation_dict_weekends_2018 = {'shp_path':shp_path, 'image_size':(1920,1080), \n",
    "                                'map_to_render':['total', 'Manhattan', 'Bronx', 'Queens', 'Staten Island', 'Brooklyn'],\n",
    "                                'render_single_borough':False,\n",
    "                                'filter_query_on_borough':False,\n",
    "                                'title':'General flow of passengers on weekends in 2018', \n",
    "                                'db':'nyc_taxi_rides', 'data_table':'taxi_rides_2018',\n",
    "                                'lookup_table':'taxi_zone_lookup_table', \n",
    "                                'aggregated_result':'count',\n",
    "                                'time_granularity':'on_specific_weekdays', \n",
    "                                'period':['2018-01-01','2018-12-31'],  \n",
    "                                'weekdays':(5, 6), 'aggregate_period':True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the base map...\n",
      "Querying the dabase...\n",
      "Rendering the results for NYC...\n",
      "The video for NYC has been rendered\n",
      "Rendering the results for Manhattan...\n",
      "The video for Manhattan has been rendered\n",
      "Rendering the results for Bronx...\n",
      "The video for Bronx has been rendered\n",
      "Rendering the results for Queens...\n",
      "The video for Queens has been rendered\n",
      "Rendering the results for Staten Island...\n",
      "The video for Staten Island has been rendered\n",
      "Rendering the results for Brooklyn...\n",
      "The video for Brooklyn has been rendered\n"
     ]
    }
   ],
   "source": [
    "make_flow_animation(animation_dict_weekends_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - render the heat maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heat maps - over the year 2018**\n",
    "\n",
    "Note that what we actually render is called a chloropeth map, and not a heat map...\n",
    "\n",
    "Let's first generate all the maps (whole city and borough focused) for the whole year 2018. We are going to generate them using the difference in the count of people.\n",
    "\n",
    "Let's identify the arguments we need to pass to the function:\n",
    "- **shp_path** (same shapefile no matter what animation we are rendering)\n",
    "- **image_size**: [1920, 1080]\n",
    "- **db**: the nyc_taxi_rides database\n",
    "- **data_table**: the preprocessed table passenger_count_2018\n",
    "- **lookup_table**: the taxi_zone_lookup_table table\n",
    "- **aggregated_result**: count\n",
    "- **weekdays_vs_weekends**: False, as we first want just the overall count on the whole year.\n",
    "- **period**: we want the whole year, so ['2018-01-01','2018-12-31']!\n",
    "- **render_single_borough**: let's set this to False, so we just focus and zoom on to the borough when rendering them separately\n",
    "- **filter_query_on_borough**: set to False so we make one query per day that we can reuse to render each animation individually\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the dabase...\n",
      "Building the outgoing maps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acoullandreau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "/Users/acoullandreau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: FutureWarning: `item` has been deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the incoming maps...\n"
     ]
    }
   ],
   "source": [
    "shp_path = \"/Users/acoullandreau/Desktop/Taxi_rides_DS/taxi_zones/taxi_zones.shp\"\n",
    "\n",
    "heat_map_dict = {'shp_path':shp_path, 'image_size':(1920,1080),'db':'nyc_taxi_rides', \n",
    "                 'data_table':'passenger_count_2018','lookup_table':'taxi_zone_lookup_table', \n",
    "                 'aggregated_result':'count', 'weekdays_vs_weekends':False,\n",
    "                 'period':['2018-01-01','2018-01-31'], 'render_single_borough':False,\n",
    "                  'filter_query_on_borough':False, 'title':'Chloropeth map over the year'} \n",
    "\n",
    "make_heat_map(heat_map_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heat maps - over the year 2018, difference between weekdays and weekends**\n",
    "\n",
    "Let's first generate all the maps (whole city and borough focused) for the whole year 2018. We are going to generate them using the difference in the count of people.\n",
    "\n",
    "The only argument that will vary now is the weekdays_vs_weekends, that we set to True to compute the difference between the flow on weekdays and on weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the dabase...\n",
      "Building the outgoing maps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acoullandreau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "/Users/acoullandreau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: FutureWarning: `item` has been deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the incoming maps...\n"
     ]
    }
   ],
   "source": [
    "heat_map_dict = {'shp_path':shp_path, 'image_size':(1920,1080),'db':'nyc_taxi_rides', \n",
    "                 'data_table':'passenger_count_2018','lookup_table':'taxi_zone_lookup_table', \n",
    "                 'aggregated_result':'count', 'weekdays_vs_weekends':True,\n",
    "                 'period':['2018-01-01','2018-01-31'], 'render_single_borough':False,\n",
    "                  'filter_query_on_borough':False,\n",
    "                 'title':'Chloropeth map of the difference of flow betwen weekdays and weekends'} \n",
    "\n",
    "make_heat_map(heat_map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this analysis was to see whether there are some trends in the flow of passengers using the NYC yellow taxis in 2018. \n",
    "\n",
    "Two blog posts were written to expose both the method exposed in this notebook and the part I notebook, as well as the results:\n",
    "- the results : https://medium.com/@mozart38/where-do-people-go-in-nyc-5-facts-and-observations-of-2018-57308d26d7c8\n",
    "- the process : https://medium.com/@mozart38/where-do-people-go-in-nyc-the-recipe-of-an-analysis-a307499013a6\n",
    "\n",
    "\n",
    "But in a nutshell, here are the observations we made:\n",
    "\n",
    "**Can we see trends in the flow of passengers in 2018?**\n",
    "\n",
    "Yes! There are some main pick up and drop off centers, among them Manhattan, the airports, and Manhattan nearby Brooklyn and Queens.\n",
    "We observed that passengers usually come to these neighborhoods from a nearby neighborhood (radius of about 34km), and tend to reach out to neighborhood within a 56km distance in most cases.\n",
    "\n",
    "**Is there a difference on holidays, hottest or coldest day of the year?**\n",
    "\n",
    "Yes! It seems like on bank holidays people take less the taxi, and that points out that probably yellow taxi passengers are mostly workers, using the taxi during working days.\n",
    "Regarding the impact off the weather, it was not evident, to the exception of the 4th of January, blizzard day that saw very little traffic.\n",
    "\n",
    "**Is there a difference between weekdays and weekends?**\n",
    "\n",
    "Yes! Weekdays appear to be much busier than weekends! This reinforces the idea that yellow taxi passengers are mostly workers.\n",
    "Besides, it looks like people actually come from further, and go further on weekends than the usual radius we mentioned in the first question.\n",
    "\n",
    "**Depending on the zone we look at, where are people most likely to come from? To go to? Is it different between weekdays and weekends?**\n",
    "\n",
    "We highlighted that some neighborhoods are strongly connected:\n",
    "- Manhattan, and in particular Upper East Side North and South\n",
    "- JFK and La Guardia airport with Manhattan for incoming traffic and the whole city for outgoing traffic\n",
    "- Manhattan close by neighborhood in Queens and Brooklyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
